---
title: A Stochastic Variational Inference Approach for Semiparametric Distributional Regression
subtitle: Final Kolloquium MSc Applied Statistic
author: Sebastian Lorek
institute: |
    | First supervisor: Prof. Dr. Thomas Kneib 
    | Second supervisor: Gianmarco Callegher
date: "2023-11-28"
format:
    beamer: 
        pdf-engine: pdflatex
        toc: true
        toc-depth: 2
        toccolor: black
        citecolor: blue
        linkcolor: darkblue
        toc-title: "Table of Contents"
        include-in-header: "tex/preamble.tex"
        slide-level: 2
        fontsize: 10pt
        latex-min-runs: 2
        cite-method: biblatex
        biblatexoptions:
        - backend=biber
        - maxbibnames=999
        - maxcitenames=2
        - uniquename=false
        - uniquelist=false
        - dashed=false
        - isbn=false
        - doi=false
        - eprint=false
        biblio-style: authoryear-icomp
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Introduction 

## Bayesian Inference 

- Focal point of interest is the posterior distribution
- General posterior specification for Bayesian regression 

\begin{align*}
    p(\thetavec|\yvec,\mathcal{D}) &= \frac{p(\yvec,\thetavec|\mathcal{D})}{p(\yvec | \mathcal{D})} \\
    &= \frac{p(\yvec|\thetavec,\mathcal{D})p(\thetavec)}{p(\yvec|\mathcal{D})} \\
    &= \frac{p(\yvec|\thetavec,\mathcal{D})p(\thetavec)}{\int p(\yvec|\thetavec, \mathcal{D})p(\thetavec) \, d \thetavec} \\
    &\propto p(\yvec|\thetavec,\mathcal{D})p(\thetavec) 
\end{align*}

- Primary challenge is the calculation of the normalizing constant/evidence 

---

- MCMC methods have been **so far** the most common methods for Bayesian inference
  - Enjoy nice properties
  - Unfortunately do not scale well for modern applications
- Trade some **accuracy** for **scalability**
- Make use of SGD and automatic differentiation 

# Theory 

## Variational Inference 

- VI is a method from machine learning to **approximate** probability densities [@Jordan1999]
- Approximate posterior with a variational distribution $q(\thetavec|\phivec)$ from a predefined parametric variational family $\mathcal{Q}$
  
\begin{align*}
    q(\thetavec|\phivec) &\in \mathcal{Q}
\end{align*}

- Use optimization (SGD) to find a member that is as closely as possible to the true posterior
- What means close in terms of distributions ?

## Kullback-Leibler divergence 

- A well known divergence measure is the Kullback-Leibler divergence [@Kullback1951]
- Quantifies the proximity between two probability distributions

\begin{align*}
    \text{D}_{\text{KL}}\left( q(\thetavec|\phivec) || p(\thetavec|\yvec,\mathcal{D}) \right) &= \int q(\thetavec|\phivec) \ln \left( \frac{q(\thetavec | \phivec)}{p(\thetavec|\yvec,\mathcal{D})} \right) \, d \thetavec \\
    &= \text{E}_{q(\thetavec|\phivec)} \left[ \ln \left( \frac{q(\thetavec|\phivec)}{p(\thetavec|\yvec,\mathcal{D})} \right) \right]
\end{align*}

- In short $\text{D}_{\text{KL}} \left(q||p\right)$
- It holds that $\text{D}_{\text{KL}}\left(q||p\right) \geq 0$
- Has some nice properties but also drawbacks (not a distance)

## Optimization objective

\begin{align*}
  \hat{\phivec} &= \argmin_{\phivec} \text{E}_{q(\thetavec|\phivec)} \left[ \ln \left( \frac{q(\thetavec|\phivec)}{p(\thetavec|\yvec,\mathcal{D})} \right) \right] \\
\end{align*}

- Make $q$ as close as possible to $p$

---

- The posterior is unknown, but the **evidence** is a **constant** in the optimization and thus cancels out 
- Allows us to rewrite the objective as 

\begin{align*}
    \hat{\phivec} &= \argmax_{\phivec} \text{E}_{q(\thetavec|\phivec)} \left[ \ln \left( \frac{p(\yvec,\thetavec|\mathcal{D})}{q(\thetavec|\phivec)} \right) \right] \\
    &= \argmax_{\phivec} \text{ELBO}(\phivec)
\end{align*}

## Evidence Lower Bound  

- What does the ELBO ?

\begin{align*}
    \text{ELBO}(\phivec) &= \text{E}_{q(\thetavec|\phivec)} \left[ \ln(p(\yvec|\mathcal{D},\thetavec)) \right] - \text{D}_{\text{KL}} \left( q(\thetavec|\phivec)||p(\thetavec) \right)
\end{align*}

- Actually something similar to MAP/ML ðŸ¤”

## Variational family 

- What is the $\mathcal{Q}$ and thus $q(\thetavec|\phivec)$ ? 
- Flexibility of $\mathcal{Q}$ and thus $q(\thetavec|\phivec)$ significantly influences the optimization 
  - Complex $\mathcal{Q}$ $\rightarrow$ better approximations, but increased complexity during optimization
  - Simple $\mathcal{Q}$ $\rightarrow$ worse approximations, but allows for easier  optimization

---

- We use structured mean-field variational inference [@Wainwright2007]
  
\begin{align*}
    q(\thetavec|\phivec) = \prod_{j=1}^{J}q_{j}(\thetavec_{j}|\phivec_{j})
\end{align*}

- One $q_{j}$ (factor) for each parameter block
- Allows for interdependencies in parameter blocks

---

- How are the $q_{j}$ defined ?

\begin{align*}
    \thetavec_{j} &= \Lbold_{j} \epsilonvec_{j} + \muvec_{j}, \; \epsilonvec_{j} \sim \mathcal{N}(\zerovec, \I) \\
    \thetavec_{j} &\sim \mathcal{N}(\muvec_{j}, \Lbold_{j} \Lbold_{j}^{\mathrm{T}})
\end{align*}

- Called "reparameterization trick" $\thetavec_{j} = \gvec_{\phivec_{j}}(\epsilonvec_{j})$
- For positive restricted parameters we need to chain another transformation layer via $\exp$ transformation

## Stochastic variational inference 

- SGD to optimize the ELBO [@Hoffman2012]
- Only use a subset $\mathcal{I}$ of the data in each iteration (ELBO remains unbiased)

## "Black-box" variational inference

- Using SVI with the "reparameterization trick"
- Evaluate the integral in the ELBO with Monte Carlo integration
- Calculate $\nabla_{\phivec} \text{ELBO}(\phivec)_{\mathcal{I}}$ with automatic differentiation
- Researcher only formulates a probabilistic model and provides a dataset [@Kucukelbir2016], inference algo. is model "agnostic"

## Optimization 

- Split data into training and validation 
- Decide for batch-size (subset of the data) and variational sample size (Monte Carlo integration)
- Optimize the ELBO using training data and monitor convergence in the validation dataset
  
\begin{align*}
    \hat{\phivec}^{t} = \hat{\phivec}^{t-1} + \rho_{t} \nabla_{\phivec} \text{ELBO}(\phivec)_{\mathcal{I}^{t}} \Bigr\rvert_{\phivec = \hat{\phivec}^{t-1}}
\end{align*}

- We use Adam [@Kingma2014] as our optimizer 

## Impact Analysis 

::: {layout="[[50,-5,50]]"}

![ELBO traces for 3 different SGD runs, using different initializations but the same seed. We use a batch size of 128, 64 samples from the variational distribution and a learning rate of 1e-2.](assets/plot2.pdf){#fig-elbo-trace1}

![ELBO traces for 3 different SGD runs, using different seeds but the same initialization. Otherwise same configuration as in @fig-elbo-trace1.](assets/plot3.pdf){#fig-elbo-trace2}

:::

---

::: {layout="[[50,-5,50]]"}

![ELBO traces for 3 different SGD runs, using different variational sample sizes. We use batch VI with a learning rate of 1e-2 and the same seed as in @fig-elbo-trace1.](assets/plot4.pdf){#fig-elbo-trace3}

![ELBO traces for 3 different SGD runs, using different batch sizes. We use a variational sample size of 64 with a learning rate of 1e-2 and the same seed as in @fig-elbo-trace1.](assets/plot5.pdf){#fig-elbo-trace4}

:::

## Semiparametric distributional regression 

\begin{align*}
    y_{i} &\ind p(\vartheta_{i, 1}, \ldots, \vartheta_{i, g}) \\
    \vartheta_{i, l} &= h^{-1}_{l}(\eta_{i, l}), \ l = 1, \ldots, g
\end{align*}

- Response distributions beyond the normal distribution
- Linear predictors with inverse link functions
- Structured addtive linear predictors so fixed and smooth effects
  - B-spline basis functions
- Augmented with priors 
  - Bayesian P-splines 

# Application

## Implementation

- Developed a small `python` package `tigerpy`, which consists of two libraries 
- A model building library `tigerpy.model`
  - Construct the model
  - Uses the idea of probabilistic graphical models 
- An inference library `tigerpy.bbvi`
  - Runs the BBVI inference algorithm 
- Aligns with concepts found in `liesel` [@Riebl2022]

## Simulation Studies 

- Conducted two simulation studies 
- First studies asymptotic behavior of the posterior means of BBVI
- Second targets the posterior distributions as a whole and compares BBVI with MCMC
  
## Consistency study 

- Study the asymptotic behavior of the posterior means 
- Performance measures include bias and empirical standard error
- Simulation repetitions: $n_{\text{sim}}=200$
- Two models
  1.  $M_{1}$: Bayesian linear regression
  2.  $M_{2}$: Bayesian logistic regression

---

- Two data generating processes, $n_{\text{obs}} = 50, 100, 500, 1000, 5000$
- For $M_{1}$ 

\begin{align*}
  y_{i} | x_{i} &\sim \mathcal{N}(3.0 + 0.2x_{i} - 0.5x_{i}^{2}, 1.0^{2}), \\
  x_{i} &\sim \mathcal{U}(-3,3), \; i = 1,\dots, n_{\text{obs}} \\
\end{align*}

- For $M_{2}$ 

\begin{align*}
  y_{i} | x_{i} &\sim \text{Bern}(\sigma(1.0 + 2.0x_{i})), \\
  x_{i} &\sim \mathcal{U}(-3,3), \; i = 1, \dots, n_{\text{obs}} \\
\end{align*}

## $M_{1}$

![Kernel density for the posterior means of the location parameters of $M_{1}$, true parameters given by $\betavec = [3.0, 0.2, -0.5]^{\mathrm{T}}$ (grey dashed line).](assets/plot7.pdf){width=80% #fig-kdeplot-loc}

---

![Kernel density for the posterior means of the scale parameter of $M_{1}$, true parameter given by $\sigma=1.0$ (grey dashed line).](assets/plot8.pdf){width=80% #fig-kdeplot-scale}

---

\begin{table}[htbp]
\centering
\caption{Simulation results for the parameters of $M_{1}$.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} & \multicolumn{5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11} 
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{-0.4819} & \textbf{-0.2319} & \textbf{-0.0520} & \textbf{-0.0162} & 0.0002 & 0.4563 & 0.3046 & 0.1036 & 0.0640 & 0.0344 \\[-4pt]
         & {\tiny(0.0323)} & {\tiny(0.0215)} & {\tiny(0.0073)} & {\tiny(0.0045)} & {\tiny(0.0024)} & {\tiny(0.0229)} & {\tiny(0.0153)} & {\tiny(0.0052)} & {\tiny(0.0032)} & {\tiny(0.0017)} \\
        $\beta_{1}$ & 0.0113 & 0.0016 & 0.0003 & 0.0021 & -0.0007 & 0.0996 & 0.0598 & 0.0300 & 0.0237 & 0.0166 \\[-4pt]
        & {\tiny(0.0070)} & {\tiny(0.0042)} & {\tiny(0.0021)} & {\tiny(0.0017)} & {\tiny(0.0012)} & {\tiny(0.0050)} & {\tiny(0.0030)} & {\tiny(0.0015)} & {\tiny(0.0012)} & {\tiny(0.0008)}\\
        $\beta_{2}$ & \textbf{0.0972} & \textbf{0.0428} & \textbf{0.0095} & \textbf{0.0033} & -0.0002  & 0.0975 & 0.0614 & 0.0264 & 0.0176 & 0.0104 \\[-4pt]
        & {\tiny(0.0069)} & {\tiny(0.0043)} & {\tiny(0.0019)} & {\tiny(0.0012)} & {\tiny(0.0007)} & {\tiny(0.0049)} & {\tiny(0.0031)} & {\tiny(0.0013)} & {\tiny(0.0009)} & {\tiny(0.0005)} \\
        $\sigma$ & \textbf{0.2389} & \textbf{0.0876} & \textbf{0.0115} & -0.0014 & -0.0021 & 0.2851 & 0.1293 & 0.0409 & 0.0300 & 0.0216 \\[-4pt]
        & {\tiny(0.0202)} & {\tiny(0.0091)} & {\tiny(0.0029)} & {\tiny(0.0021)} & {\tiny(0.0015)} & {\tiny(0.0143)} & {\tiny(0.0065)} & {\tiny(0.0021)} & {\tiny(0.0015)} & {\tiny(0.0011)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}% 
}
\end{table}

## $M_{2}$

![Kernel density for the posterior means of the logit parameters of $M_{2}$, true parameters given by $\betavec = [1.0, 2.0]^{\mathrm{T}}$ (grey dashed line).](assets/plot9.pdf){width=80% #fig-kdeplot-logit}

---

\begin{table}[htbp]
\footnotesize
\centering
\caption{Simulation results for the parameters of $M_{2}$.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} &\multicolumn {5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11}
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{0.1904} & 0.0631 & 0.0044 & \textbf{-0.0481} & 0.0137 & 0.7180 & 0.5568 & 0.2632 & 0.2072 & 0.1103 \\[-4pt]
        & {\tiny(0.0508)} & {\tiny(0.0394)} & {\tiny(0.0186)} & {\tiny(0.0147)} & {\tiny(0.0078)} & {\tiny(0.0360)} & {\tiny(0.0279)} & {\tiny(0.0132)} & {\tiny(0.0104)} & {\tiny(0.0055)} \\
        $\beta_{1}$ & \textbf{0.3578} & \textbf{0.1914} & \textbf{0.0354} & 0.0111 & \textbf{0.0368} & 0.9137 & 0.6532 & 0.2905 & 0.2349 & 0.1167 \\[-4pt]
        & {\tiny(0.0646)} & {\tiny(0.0462)} & {\tiny(0.0205)} & {\tiny(0.0166)} & {\tiny(0.0083)} & {\tiny(0.0458)} & {\tiny(0.0327)} & {\tiny(0.0146)} & {\tiny(0.0118)} & {\tiny(0.0058)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}%
}
\end{table}

## Posterior density study 

- Estimate a smooth function through Bayesian P-splines 
- Compare posterior distributions of BBVI (`tigerpy`) and MCMC (`liesel` [@Riebl2022])
- For comparison we use:
  1. Kernel density plots 
  2. Wasserstein distance [@Kantorovich1960]
- Generate 4 MCMC Chains and 400 ($n_{\text{sim}}$) BBVI runs
- Data generating process (DGP)

\begin{align*}
    y_{i} | x_{i} &\sim \mathcal{N}(f(x_{i}), 1.5^{2}) \\
    f(x_{i}) &= 3.0 + 1.75 \sin(1.5x_{i}) \\
    x_{i} &\sim \mathcal{U}(-10,10), \ i=1, \dots, 1000,
\end{align*}

---

![The DGP and the estimated smooth functions from BBVI and MCMC, using the posterior means.](assets/plot10.pdf){width=80%}

---

![Kernel density for the posterior samples of the fixed intercept $\beta_{0}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plot11.pdf){width=80%}

---

![Kernel density for the posterior samples of selected internal spline coefficients $\tilde{\gammavec}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plot12.pdf){width=80%}

--- 

![Kernel density for the posterior samples of the inverse smoothing parameter $\tau^{2}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plot13.pdf){width=60%}

---

![Kernel density for the posterior samples of the scale $\sigma$, using 4 randomly slected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plot14.pdf){width=80%}

---

- As a formal measure we use the Wasserstein distance with the "squared euclidean distance" ($W_{2}$)
- Allows to compare the "distance" between two probability distributions

---

![Box plots displaying the Wasserstein distance for the different model parameters.](assets/plot15.pdf){width=80%}

## Open Problems 

- Starting with "arbitrary" model initializations for models with scale or shape parameters is numerically too unstable
- Likelihood in the ELBO tends to infinity for "unlikely" samples from the variational distribution
- Forcing the variance of the variational distribution down works ...

---

![Location scale regression with the Dutch boys dataset, comparing MCMC and BBVI.](assets/plot_dbbmi.pdf){width=80%}

---

- Currently working on a two stage procedure 
  1. Start with MAP for a few iterations 
     - Caculate Laplace approximation and use it as the initialization for BBVI
  2. Continue with BBVI 
- Optional to include one further simulation study or compare results from the first study with MCMC