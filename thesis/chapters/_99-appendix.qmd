---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Appendices {.unnumbered}

\renewcommand{\thesubsection}{\Alph{subsection}}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}

## Robbins–Monro algorithm {#sec-r-m-algo}

The sequence of step sizes $\rho_{t}$ must satisfy

$$
\begin{split}
    \sum_{t=1}^{\infty} \rho_{t} = \infty \\
    \sum_{t=1}^{\infty} \rho_{t}^{2} < \infty,
\end{split}
$$

such that $\phivec_{t}$ will converge to the optimal $\hat{\phivec}$ either local or global depeding if the objective function is non-convex or convex [@Robbins1951]. Because the ELBO is a non-convex function we will converge to a local optimum. With the aim of finding a local optimum, the ELBO must exhibit three-times differentiability and satisfy a set of mild technical requirements, as outlined by @Bottou1999, conditions that are met by our variational objective [@Hoffman2012, p.1318]. These conditions ensure that every configuration within the parameter space is attainable, with the added assurance that gradient noise diminishes rapidly, promoting convergence [@Zhang2017, p. 5].

## Score gradient estimator {#sec-deriv-score-grd}

The following derivation shows how to derive the score gradient estimator from @Ranganath2014. In the last step we can use Monte Carlo integration to approximate the derivate.

$$
\begin{split}
    \nabla_{\phivec}\text{ELBO}(\phivec) &= \nabla_{\phivec} \int q(\thetavec | \phivec) \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \,d \thetavec \\
    &= \int (\nabla_{\phivec} q(\thetavec | \phivec)) \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \,d \thetavec + \int q(\thetavec | \phivec) \left( \nabla_{\phivec}\ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \right) \,d \thetavec \\
    &= \int (\nabla_{\phivec} q(\thetavec | \phivec)) \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \,d \thetavec - \int q(\thetavec | \phivec) (\nabla_{\phivec} \ln [q(\thetavec | \phivec)]) \,d \thetavec \\
    &= \int q(\thetavec | \phivec) \frac{(\nabla_{\phivec} q(\thetavec | \phivec))}{q(\thetavec | \phivec)} \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \,d \thetavec - \int q(\thetavec | \phivec) \frac{(\nabla_{\phivec} q(\thetavec | \phivec))}{q(\thetavec | \phivec)} \,d \thetavec \\
    &= \int q(\thetavec | \phivec) (\nabla_{\phivec} \ln [q(\thetavec | \phivec)]) \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \,d \thetavec - \int \nabla_{\phivec} q(\thetavec | \phivec) \,d \thetavec \\
    &= \int q(\thetavec | \phivec) (\nabla_{\phivec} \ln [q(\thetavec | \phivec)]) \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \,d \thetavec - \nabla_{\phivec} \int q(\thetavec | \phivec) \,d \thetavec \\
    &= \text{E}_{q(\thetavec | \phivec)} \left[ (\nabla_{\phivec} \ln [q(\thetavec | \phivec)]) \ln \left[ \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right] \right]\\
    &\approx \frac{1}{S} \sum_{s=1}^{S} (\nabla_{\phivec} \ln [q(\thetavec | \phivec)]) \Bigr\rvert_{\thetavec = \thetavec^{s}} \ln \left[ \frac{p(\yvec, \thetavec^{s} | \X)}{q(\thetavec^{s} | \phivec)} \right], \; \thetavec^{s} \sim q(\thetavec | \phivec).
\end{split}
$$

### Control Variates {#sec-contr-var}

Because it holds that

$$
\int q(\thetavec | \phivec) \nabla_{\phivec} \ln q(\thetavec | \phivec) \,d \thetavec = 0,
$$

we can add/subtract any multiple $C$ of this expression to the score gradient estimator.

$$
\begin{split}
    \nabla_{\phivec} \text{ELBO}(\thetavec | \phivec) &= \text{E}_{q(\thetavec | \phivec)} \left[ \nabla_{\phivec} \ln q(\thetavec | \phivec) \left( \ln \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} - C \right) \right] \\
    &\approx \frac{1}{S} \sum_{s=1}^{S} \nabla_{\phivec} \ln q(\thetavec_{s} | \phivec) \left( \ln \frac{p(\yvec, \thetavec_{s} | \X)}{q(\thetavec_{s} | \phivec)} - C \right), \; \thetavec_{s} \sim q(\thetavec | \phivec).
\end{split}
$$

Such that we can reduce the variance of the gradient estimator without changing the expectation value. The $C$ can be for example defined as

$$
C = \frac{1}{S} \sum_{s=1}^{S} \ln \frac{p(\yvec, \thetavec_{s} | \X)}{q(\thetavec_{s} | \phivec)}.
$$

## Model construction example {#sec-m-constr-ex}

This example outlines how to construct a Bayesian linear regression model in `tigerpy`.

```python
import tigerpy.model as tiger
import numpy as np
import tensorflow_probability.substrates.jax.distributions as tfjd

# Set up design matrix 
X = tiger.Obs(name="X_loc")
X.fixed(data = np.ndarray)

# Set up hyperparameters
beta_loc = tiger.Hyper(0.0, 
                       name="beta_loc")
beta_scale = tiger.Hyper(100.0, 
                         name="beta_scale")

# Set up parameters
beta_dist = tiger.Dist(tfjd.Normal, 
                       loc=beta_loc, 
                       scale=beta_scale)
beta = tiger.Param(value=np.array([0.0, 0.0, 0.0]), 
                   distribution=beta_dist, 
                   name="beta")

# Set up hyperparameters for the scale
sigma_a = tiger.Hyper(0.01, 
                      name="sigma_a")
sigma_b = tiger.Hyper(0.01, 
                      name="sigma_b")

sigma_dist = tiger.Dist(tfjd.InverseGamma, 
                        concentration=sigma_a, 
                        scale=sigma_b)

# Use paramter param_space="positive" to transform 
# sigma into unconstrained space  
sigma = tiger.Param(value=10.0, 
                    distribution=sigma_dist, 
                    param_space="positive", 
                    name="sigma")

# Set up the linear predictor
lpred = tiger.Lpred(obs=X, 
                    beta=beta)

# Set up response distribution
response_dist = tiger.Dist(tfjd.Normal, 
                           loc=lpred, 
                           scale=sigma)
m = tiger.Model(response=np.ndarray, 
                distribution=response_dist)
```