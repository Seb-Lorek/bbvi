---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Implementation {#sec-impl}

In this chapter, we will introduce the software package `tigerpy`, which has been developed for this master's thesis. It implements BBVI for Bayesian semiparametric distributional regression models within the framework of probabilistic graphical models. The implementation is written in Python [@python3] and is structured as a package to enhance the reusability of the code. `tigerpy` consists of a model building library, `tigerpy.model`, as well as an inference library, `tigerpy.bbvi`, which allows running the inference algorithm on a constructed probabilistic graphical model. The process of constructing models in `tigerpy` closely mirrors and aligns with principles found in the Python package `liesel` [@Riebl2022].

## Model building library {#sec-m-lib}

The model building library `tigerpy.model` allows for flexible model formulation and ultimately generates a DAG. The library consists of several classes that serve as fundamental model building blocks, which can be stacked in a mathematically valid order. The first important class for model construction is the `tigerpy.model.Obs` class, which enables the construction of an instance of a design matrix for a linear predictor node. The following code demonstrates how to set up a design matrix for fixed coefficients only.

```python
import tigerpy.model as tiger
import numpy as np

X = tiger.Obs(name="X", 
              intercept=True)
X.fixed(data=np.ndarray)
```

We usually import the model building library with the alias `tiger`. The argument `intercept` indicates whether an intercept should be added to the design matrix. After creating an instance, we pass the data to the `.fixed()` method using a 2-D `numpy.ndarray` containing a covariate design matrix. Because we are interested in semiparametric distributional regression, the class `Obs` also has a method to construct smooth effects via `.smooth()`, utilizing the B-spline basis (see @sec-semi-para). The B-splines basis matrices are constructed using `scipy.interpolate.Bspline` from @scipy. Integrating both fixed and several smooth effects in a linear predictor, requires the use of centering constraints detailed in @sec-semi-para. To construct a design matrix for a linear predictor that contains fixed as well as smooth effects, the user must execute the following code.

```python
X = tiger.Obs(name="X", intercept=True)
X.fixed(data=np.ndarray)
X.smooth(data=np.ndarray, n_knots=20, degree=3, rwk=2)
X.center()
```

The data input for the smooth effect should be a 1-D `numpy.ndarray`. Fixed effects must be defined first. Attempting to specify fixed effects after a smooth effect has already been defined will result in an error being rased in the program. This choice was made to simplify bookkeeping within the `Obs` class. Finally, it is important to call the method `.center()` to include the centering constraints for the identifiability of the intercept and the smooth effects (see @sec-semi-para).

After defining the design matrices for the linear predictors we start building the model by using the classes: `tigerpy.model.Hyper`, `tigerpy.model.Dist`, `tigerpy.model.Param`, `tigerpy.model.Lpred`, $\;$ `tigerpy.model.Calc` and `tigerpy.model.Model`. 

A hyperparameter node is implemented by the class `Hyper`, which only has a value and name attribute. Names are important in `tigerpy` as they uniquely define a node in the model and afterwards in the graph that is constructed from the model. 

The `Dist` class constructs a probability distribution for a probabilistic node. One passes the distribution and its parameters as arguments. Here, for each parameter of the distribution, the left-hand assignment must match the corresponding parameter argument of the `tensorflow_probability.substrates.jax.distributions` application programming interface (API) [@tensorflow]. The right-hand side can be an instance of one of the following classes, depending on the context: `Hyper`, `Param`, `Lpred` or `Calc`. An instance of the `Dist` class is used either in a parameter node (`Param`) or in the response node (`Model`) to define the distribution of the probabilistic nodes. 

The `Param` class is responsible for constructing parameter nodes within the probabilistic graphical model. Parameter nodes are the nodes on which we want to conduct inference later on. They are constructed by providing an initial value, an instance of the `Dist` class, the parameter space (either `None` or `"positive"`) and their name. Because parameter nodes are stochastic, each node possesses a `.log_prior` attribute representing the log-probability density of that node. In the current implementation `tigerpy` only allows for the construction of continuous unconstrained or continuous positive constrained parameters.

Instances of the `Obs` and `Param` classes are the essential inputs for the `Lpred` class, which creates a linear predictor node. One must also supply a function which specifies the inverse link function that connects the linear predictor to a parameter of the response. The `function` argument defaults to `None`, corresponding to the identity (inverse) link. Emphasizing the importance of ordering within this context is crucial once more.  The user must supply all parameters in the order corresponding to their construction in the design matrix. This means the parameter nodes of the fixed coefficients precede smooth coefficients.

Another building block for model construction is the `Calc` class. This class allows reparameterizing the response distribution in terms of location, scale and shape. An example here is Gamma location-scale regression, where we have to reparameterize $\alpha$ (shape or concentration) and $\beta$ (rate) in terms of the location and scale. We initialize this class by passing a transformation function through the `function` argument and all linear predictors (class `Lpred`), that we use for the reparameterization.

Finally, the whole model is assembled together in the `Model` class, which creates the response node. Once again, we provide the `Dist` class as an argument to define its probability distribution. Additionally, we pass the response data via the corresponding response argument, requiring a 1-D `numpy.ndarray`. The instance of the `Model` class contains the full model, created by the hierarchical stacked instances of the classes. For a complete example of how to construct a model, refer to [Appendix @sec-m-constr-ex] for the case of a Bayesian linear regression model. The attributes `.log_lik` and `.log_prior` hold the corresponding values of the model log-likelihood and the model log-prior probability density for the initial values supplied. The `.log_prob` attribute stores the log-joint probability density of the model. 

Once we have fully constructed the model, we build a DAG from the implicit structure in the `Model` instance by using the `ModelGraph` class. The following code snipped shows how to easily build the probabilistic graphical model. 

```python
graph = tiger.ModelGraph(model=tiger.Model)
graph.build_graph()
```

The `.build_graph()` method constructs the DAG by iteratively traversing through the nested class instances, generating the relevant nodes within the graph. Under the hood, the DAG is constructed using the `networkx` package [@networkx] and its functionalities, employing the `DiGraph` class. @fig-class-diag lists the different node types and their attributes, which are the building blocks of the underlying DAG.

![Diagram illustrating various node types and their potential dependencies in the DAG generated by `tigerpy`.](assets/plots/plot7.pdf){width=80% fig-scap="Node types in the DAG constructed by `tigerpy`." #fig-class-diag}

The implementation constructs a DAG consisting of 6 different node types, each with distinct attributes (shown in the middle of the rectangles in @fig-class-diag). Additionally, each node stores information on the specific `node_type`. Furthermore, the graph's nodes are connected by directed edges that retain information on the parent-child relationship. @fig-class-diag also illustrates the potential edges that a node can have. For instance, a `strong_node` only shares directed edges with either a `root`, `lpred_node` or another `strong_node`. The `ModelGraph` class additionally provides the method `.visualize_graph()`. allowing for a clear presentation of the DAG. The visualization is inspired by plate notation of Bayesian networks. For an example of a plate diagram, refer to @fig-plate-bsemi-normal-reg. @fig-dag-ls-reg displays the graph for a Bayesian normal location-scale regression model. In this visualization, blue nodes represent probabilistic nodes, while fixed data and hyperparameter nodes constitute the leaves of the graph. Inverse link functions are shown on the edge from linear predictor nodes, unless they correspond to `None`.

![The DAG visualization for location-scale regression from the method `.visualize_graph()`.](assets/plots/plot6.pdf){width=80% fig-scap="DAG visualization for location-scale regression." #fig-dag-ls-reg}

Once we have build the probabilistic graphical model with `.build_graph()`, we can proceed to the inference library `tigerpy.bbvi` to run the inference algorithm on the probabilistic graphical model. In its current state, `tigerpy` provides validated support for various response distributions, including Normal, Bernoulli, Poisson, and Gamma, allowing modeling for both location and scale (if applicable). Given the package's adaptable structure, integrating additional support for further distributions should require minimal effort. The only requirement is that the response distribution is implemented in the `tensorflow_probability.substrates.jax.distributions` API.

## Inference library {#sec-inf-lib}

Inference in `tigerpy` is enabled by its inference library `tigerpy.bbvi`. Essentially, the package constructs a variational distribution from the model graph and offers the `.run_bbvi()` method to execute the inference algorithm on the model. Initially, the user must instantiate the `tigerpy.bbvi.Bbvi` class.

```python
import tigerpy.bbvi as bbvi

q = bbvi.Bbvi(graph=tiger.ModelGraph, 
              jitter_init=True, 
              pre_train=False, 
              loc_prec=1.0, 
              scale_prec=2.0)
```

Here, we typically import the inference library under the alias `bbvi`. The class first obtains all data vectors and matrices from the graph, storing them in a dictionary. Hence, it gathers the response data and all design matrices for the linear predictors. After that, it initializes the variational parameters in $\phivec$ and stores them in a dictionary. Regarding the initialization, we can choose to set the arguments `jitter_init` and `pre_train` to either `True` or `False`. Using the `jitter_init` argument, we have the option to introduce jitter (small random noise) to the initial variational parameters. The `pre_train` argument determines whether the algorithm should utilize a pre-training phase by using maximum a posteriori (MAP) optimization, as long as the change in log-joint model density is above a threshold. This enables a "black-box" initialization strategy, particularly beneficial when predicting beyond the response distribution's location, addressing potential numerical instability arising from arbitrary variational parameter initializations. We use the optimized mode as the mean of the variational distribution and calculate the negative Hessian at these values to obtain an initialization for $\Lbold_{j}$. If the argmuent is set to `False`, the program resorts to the initial values supplied during model construction, setting the mean of the variational distribution to these values. Bear in mind that the initial values of positive constrained parameters are transformed into the unconstrained space first, due to optimizing the parameters of the variational distribution in the unconstrained space. To initialize the lower diagonal matrix of the precision matrix for a factor of the variational distribution, users are permitted to provide `loc_prec` and `scale_prec` with a float, determining the initial diagonal values of the lower triangular matrix $\Lbold_{j}$. This applies to parameter nodes on the parent branch of a linear predictor node that models location or scale parameters of the response, as well as parameters that directly model location or scale parameters of the response. Finally, we can run the inference algorithm on the instance of `Bbvi` with the method `.run_bbvi()`.

```python
import jax 

q.run_bbvi(key=jax.random.PRNGKey(42),
           learning_rate=1e-3, 
           pre_train_learning_rate=1e-2,
           grad_clip=1, 
           threshold=1e-2, 
           pre_train_threshold=0.1,
           batch_size=64, 
           pre_train_batch_size=64,
           train_share=0.8, 
           num_var_samples=32, 
           chunk_size=1, 
           epochs=250)
```

The method requires several arguments that specify the various parameters essential for configuring the inference algorithm. For details on the arguments, consult @sec-inf-alg (Algorithm \ref{algo}) or the docstrings on GitHub. We only highlight the arguments `key` and `chunk_size`. The `key` defines the starting key for pseudo-random number generation in `JAX`. `chunk_size` determines the behavior of the `jax.lax.scan` function, which internally calls the function `epoch_body()` for the execution of one epoch iteration. Setting the `chunk_size` to 1 results in the behaviour of a usual `for` loop with `epochs` iterations, while an `int` > 1 results in a scaned execution of the code^[Consult the `JAX` documentation ([https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html)) for details.]. This aids in compilation speed, as a standard `for` loop in just-in-time (JIT)-compiled functions introduces a significant computational overhead.

After the program has executed the `.run_bbvi()` method, the instance of `Bbvi` retains the optimized variational parameters in the unconstrained space within the `.var_params` attribute. The optimized variational parameters in the constrained parameter space are stored in the attribute `.trans_var_params`. Subsequently, users can utilize the `.plot_elbo()` and `.get_posterior_samples()` methods. These methods facilitate either the plotting of ELBO convergence in the validation set or the retrieval of posterior samples for each parameter block from the variational distribution.

## Technology {#sec-technology}

As with all numerical computationally intensive algorithms, achieving satisfactory performance in terms of computation time is crucial. For fast execution of the Algorithm \ref{algo} outlined in [Chapter @sec-svinf], efficient linear algebra computations are esssential. Furthermore, we need to be able to use automatic differentiation on the ELBO and also fast random number generation. `JAX` [@jax] is a high-performance numerical computing library that provides the necessary functionalities. It enables JIT compilation for Python code, excelerating the performance of computationally demanding programs by using the accelerated linear algebra (XLA) compiler [@tensorflow]. Its API closely resembles what researchers are accustomed to with `numpy` [@numpy]. Theoretically, one could enhance scalability further by executing the program on different backends, e.g. on a graphical processing unit (GPU) or a tensor processing unit (TPU), instead of a central processing unit (CPU), without altering the underlying code.

For gradient processing and optimization, we use the library `optax` [@optax], which is exclusively designed for `JAX`. In detail, we use either `optax.adam` or `optax.adamw`, depending on whether the `learning_rate` is a `schedule` or not. The function computes the parameter updates from their gradients for each optimization step. Furthermore, as we use gradient clipping (`.clip()`) to enhance numerical stability during optimization, we make use of `optax`s `.chain()` to apply chainable update transformations.

As previously mentioned in @sec-m-lib, we employ the `networkx` [@networkx] `DiGraph` class for constructing, traversing, and visualizing the DAG. The class `DiGraph` allows to store nodes and directed edges with optional data. A node can be a arbitrary Python object with optional key and value attributes. Directed edges are links between the nodes that can also store data through key and value attributes. The class also provides several algorithms for DAGs. In our case, we especially make use of the `topological_sort()` algorithm to obtain a traversing order that respects the dependencies of the DAG. 

Although we have not implemented any unit or integration tests thus far, the code features extensive docstrings that help in comprehending the intended usage of arguments and functions.