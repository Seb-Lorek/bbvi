---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Bayesian hierarchical distributional modelling {#sec-bhm}

This Chapter introduces the class of Bayesian semiparametric distributional regression models, being a important subcategory of Bayesian hierarchical models, for which we want to asses the statistical properties of BBVI in simulation studies in [Chapter @sec-sim]. Moreover we consider how to integrate this model class into the framework of probabilistic graphical models, also referred to as Bayesian networks. In Bayesian statistics, we explicitly express our assumptions not only concerning the data through the likelihood function but also consider the model parameters as random variables with associated prior probability distributions. This means we are fully transparent on the distributional assumptions in our statistical model. As a results we obtain a hierarchical dependency structure in our model, explaining the usage of the term.

## Distributional regression {#sec-dist-reg}

Generalized linear models (GLM) unify all regression models that relate a linear predictor to the mean of the response, with a specified link function, for a parametric conditional response distribution [@Fahrmeir2021, p. 623]. A well known model that belongs to the class of GLMs which we already introduced in this thesis is the logistic regression model (@sec-binf). Distributional regression goes one step further and relates covariates to further moments of the conditional response distribution [@Fahrmeir2021, chap. 10]. E.g., for a conditional normally distributed response it might be of interest to also model the scale (standard deviation) in addition to the location (mean), due to heteroscedastic residuals when using a location only specification, giving us the well known location-scale regression model^[Due to notational convenience we omit the dependence of the response on the covariates (data) in the following.]

$$
\begin{split}
    y_{i} &\ind \mathcal{N}(\mu_{i},  \sigma_{i}^{2}) \\
    \mu_{i} &= \eta_{i, 1} \\
    \sigma_{i} &= \exp(\eta_{i, 2}).
\end{split}
$$ {#eq-loc-scale}

The linear predictors $\eta_{i, l}, \ l = 1,2$ contain the effects of the covariates. Hence, such models allow the researcher to study the impact of covariates beyond the mean [@Fahrmeir2021, p. 623]. This can be motivated by modelling the response distribution more accurately or by focusing on certain properties of the distribution guided by a specific research question. It is also possible to extend the analysis beyond traditional location-scale regression by modelling moments such as skewness and kurtosis. To achieve this, one may opt for a more versatile distribution in lieu of the normal distribution. Generalized additive models for location, scale and shape (GAMLSS) provide a unifying framework for distributional regression, first proposed by @Rigby2005. Here we allow for more flexible response distributions and potentially model any of the parameters that parameterize the distribution [@Fahrmeir2021, p. 641]. Furthermore the researcher can easily make use of structured additive forms in the linear predictors in this framework. E.g. the linear predictors in (@eq-loc-scale) might combine fixed and smooth terms such that we obtain the following additive structure 

$$
\eta_{i, l} = \xvec_{i}^{\mathrm{T}} \betavec^{l} + f_{1}^{l}(\nu_{i, 1}) + \ldots + f_{p}^{l}(\nu_{i, p}).
$$ {#eq-lpred}

$\nu_{i,1}, \ldots, \nu_{i, p}$ are the covariates that we model with a non-linear function $f$. Bear in mind that we can include different sets of covariates in each linear predictor $l$. The choice of the function $f$ that models the smooth effects will be dealt with in @sec-semi-para. A general model specification of GAMLSS, in the case of independent response data, is given by

$$
\begin{split}
    y_{i} &\ind p(\vartheta_{i, 1}, \ldots, \vartheta_{i, g}) \\
    \vartheta_{i, l} &= h^{-1}_{l}(\eta_{i, l}), \ l = 1, \ldots, g.
\end{split}
$$

To appropriately respect the parameter space of $\vartheta_{i, l}$, we employ a monotonic inverse link function $h^{-1}_{l}$ [@Fahrmeir2021, p. 645]. Traditionally inference in GAMLSS relied either on the Rigby and Stasinopoulos (RS) or Cole and Green (CG) algorithm [@Rigby1996; @Rigby2005] or MCMC methods in a Bayesian distributional regression setup [@Klein2015; @Umlauf2018]. Modern inference methods, such as BBVI, may offer intriguing use cases by enabling the expansion of model complexity and scalability to large data sets. In Bayesian semiparametric distributional regression, we naturally augment all parameters with their respective priors.

## Nonparametric covariate effects {#sec-semi-para}

In the context of Bayesian semiparametric distributional regression we can flexibly combine fixed and smooth effects in the linear predictor. This may be especially admissible when we expect that the relationship between a subset of covariates and the reponse is highly non-linear and difficult to model using simple polynomial transformations only. For such scenarios, @Eilers1996 proposed a solution using penalized basis Splines (P-Splines), which allow us to flexibly model the relationship at hand but to maintain a smooth fit at the same time. @Lang2004 extended this idea to a Bayesian approach, which offers an alternative possibility of estimating the coefficients and finding a suitable smoothing parameter. P-splines are especially appealing as they allow to estimate a non-linear and smooth function $f$ in the linear predictor relatively parsimoniously [@Lang2004, p. 184]. To begin with, the covariate domain is divided into distinct intervals, which are separated by equally spaced knots. The usual choice is to use 20-40 knots and use the (inverse) smoothing parameter to enforce a smooth fit. In a next step, basis functions are placed across the intervals using basis spline (B-spline) functions of degree $l$. To obtain a twice differentiable basis function it is common to use a degree of 3, for an in depth definition of the B-spline basis consult @Fahrmeir2021 [pp. 445 ff.]. Using a weighted linear combination of $d$ B-spline basis functions we obtain the following specification for the smooth effect function

$$
f(\nu_{i}) = \sum_{j=1}^{d} \gamma_{l} B_{j}(\nu_{i}).
$$

The covariate gets evaluated at all basis functions $B_{j}(\nu_{i})$, giving us a design matrix $\Z$, with the following structure

$$
\Z = \begin{bmatrix} 
     B_{1}(\nu_{1}) & \cdots & B_{d}(\nu_{1}) \\
     \vdots & \ddots & \vdots \\
     B_{1}(\nu_{n}) & \cdots & B_{d}(\nu_{n})
     \end{bmatrix}.
$$

To obtain a smooth fit in a Bayesian setting we replace the squared difference penalty from P-spline estimation with appropriate random walk priors for the coefficients, which are the stochastic analogues. Let $\gammavec = \left(\gamma_{1}, \ldots, \gamma_{d}\right)^{\mathrm{T}}$ denote the vector of weighting coefficients for a smooth effect, for which we define a random walk prior of difference order k. If we condition $\gamma_{j}$ on its preceding coefficients and consider a random walk of order $k = 1,2$, we can define the following prior distribution using the Markov property 

$$
\begin{split}
    \gamma_{j} | \gamma_{j-1} &\sim \mathcal{N}(\gamma_{j-1}, \tau^{2}), \ \text{for} \ k = 1 \\
    \gamma_{j} | \gamma_{j-1}, \gamma_{j-2} &\sim \mathcal{N}(2\gamma_{j-1} - \gamma_{j-2}, \tau^{2}), \ \text{for} \ k = 2.
\end{split}
$$

Since we usually set $k=1$ or $k=2$, we assume non-informative priors for the first and potentially second coefficient 

$$
\begin{split}
    p(\gamma_{1}) &\propto const, \ \text{for} \ k = 1, 2 \\
    p(\gamma_{2}) &\propto const, \ \text{for} \ k = 2.
\end{split}
$$

From this prior specification we can also obtain the joint multivariate prior of $\gammavec$, conditional on the variance parameter $\tau^{2}$

$$
p(\gammavec | \tau^{2}) = (2 \pi)^{-\frac{r \left(\K \right)}{2}} \det \left( \frac{1}{\tau^2}K \right)_{+}^{\frac{1}{2}}\exp \left( -\frac{1}{2} \gammavec^{\mathrm{T}}\frac{1}{\tau^{2}} \K \gammavec \right)
$$

The rank deficient precision matrix $\frac{1}{\tau^{2}} \K$ results from the matrix multiplication $\frac{1}{\tau^{2}} \D^{\mathrm{T}}\D$, where $\D$ denotes the difference matrix for the k-th order differences of the coefficients [@Fahrmeir2021, pp. 455-456]. This distribution is a degenerate multivariate normal distribution, leading to an improper prior. Moreover $\det(\cdot)_{+}$ denotes the pseudo determinant, due to the precision matrix being rank deficient. To complement the prior specification we use the inverse gamma distribution as a prior^[We avoid using the term "hyperprior" to prevent unnecessary complexity in our terminology.] on the parameter $\tau^{2}$

$$
\tau^{2} \sim \text{InvG}(a,b),
$$

where $a$ and $b$ are hyperparameters that need to be set. @Lang2004 [p. 187] propose to use $a = 1$ and $b = 5\text{e-}3/5\text{e-}4/5\text{e-}5$. The parameter $\tau^{2}$ acts as an inverse smoothing parameter where large values imply flexible function shapes, while small values imply smooth function shapes. Because we set a probability distribution on this parameter we also infer the posterior distribution of this parameter during inference. To ensure the identifiability of all parameters in the additive linear predictor, we need to include centering constraints for the smooth functions $f$ during optimization in BBVI. This is achieved by using a sum-to-zero constraint for each smooth function $\sum_{i=1}^{n} f(\nu_{i}) = 0$ [@Wood2017, pp. 175-176]. A common choice is to use the QR-decomposition on the column means of the matrix $\Z_{n \times d}$. From this we obtain a matrix $\T_{d \times d-1}$, being the unitary matrix from the QR-decomposition of $\bar{\zvec}_{d \times 1}$ excluding the first column. This matrix is used to transform covariate design matrix to $\Z_{n \times d} \T_{d \times d-1}$ and the penalty matrix to $\T^{\mathrm{T}}_{d-1 \times d} \K_{d \times d} \T_{d \times d-1}$, such that the linear constraint is satisfied. As a byproduct, we reduce the dimension of the internal parameter $\tilde{\gammavec}_{d-1 \times 1}$ for each smooth effect by 1. We can however recover $\gammavec_{d \times 1}$ easily through the following linear transformation

$$
\gammavec_{d \times 1} = \T_{d \times d-1} \tilde{\gammavec}_{d-1 \times 1},
$$

for more comprehensive information refer to @Wood2017 [p. 211]. At last we can rewrite the additive linear predictors from (@eq-lpred) into a vectorized form including the sum to zero constraint

$$
\begin{split}
\etavec_{l} &= \X_{l} \betavec_{l} + \Z_{1, l}\gammavec_{1, l} + \ldots + \Z_{p, l} \gammavec_{p, l} \\
&= \X_{l} \betavec_{l} + \Z_{1, l} \T_{1, l} \tilde{\gammavec}_{1, l} + \ldots + \Z_{p, l} \T_{p, l} \tilde{\gammavec}_{p, l}.
\end{split}
$$

Being in a Bayesian setting we supplement the fixed regression coefficients with the following common prior  

$$
\betavec \sim \mathcal{N}(\zerovec, \sigma^{2} \I).
$$

 A common choice is to set $\sigma=100$. Moreover, we specify each of the parameters $\gammavec_{1,l}, \ldots, \gammavec_{p, l}$ with the prior specification $p(\gammavec | \tau^{2})p(\tau^{2})$ outlined above. 

## Probabilistic graphical models {#sec-pgm}

Probabilistic graphical models, also known as Bayesian networks, represent a framework originating from the computer science realm that enables the scaling of Bayesian models. Whereas classical statistics typically handles models with limited complexity and parameter size, contemporary machine learning applications often involve highly intricate model structures. Consequently, there arises a need for more comprehensive frameworks to facilitate the construction and inference of models at scale. In our scenario, this entails the translation of Bayesian (hierarchical) distributional regression models into this framework, thereby facilitating the construction and inference processes for these models. A probabilistic graphical model is a probability distribution for the random variables $\{x_{1}, \ldots, x_{n}\}$ that can be factorized in the following way

$$
p(x_{1}, \ldots, x_{n}) = \prod_{i=1}^{n}p(x_{i}| \text{pa}(x_{i})).
$$

In this context $\text{pa}(x_{i})$ denotes the parents of the variable $x_{i}$ and therefore the variables $x_{i}$ depends on. The dependence structure in a probabilistic graphical model lends itself to a representation as a directed acyclic graph (DAG), where every variable in the joint distribution obtains a node and parents are connnected to childs through arrows. When formulating a Bayesian hierarchical model, we make a priori dependence decisions regarding the likelihood and all associated priors. Thus we set the joint distribution of the model to a specific factorized form, which in turns implies a hierarchical structure. The hierarchical structure comes from the fact that conditional distributions dictate the dependencies between random variables and hence the flow of information in our statistical model. This furthermore allows to naturally translate Bayesian hierarchical models into the framework of probabilistic graphical models. @fig-plate-bsemi-normal-reg visualizes the DAG, through plate notation, for a Bayesian semiparameteric normal regression model with a set of fixed covariate and one smooth covariate.

```{r, engine='tikz'}
#| label: fig-plate-bsemi-normal-reg
#| fig-cap: "Plate notation of the dependence structure in a Bayesian semiparametric normal (location only) regression model."
#| fig-scap: "Plate notation of the Bayesian semiparametric normal regression model."

\usetikzlibrary{bayesnet}

% Plate diagram
\begin{tikzpicture}
    % Define nodes
    \node[obs] (y) {$y_{i}$};
    \node[const, above=of y, xshift=-1cm] (loc) {$\eta_{i}$};
    \node[const, above=of loc, xshift=-2cm] (x) {$\mathbf{x}_{i}$};
    \node[latent, above=of loc, xshift=-1cm, yshift=0.5cm] (beta) {$\boldsymbol{\beta}$};
    \node[const, above=of beta, xshift=-1cm] (beta_loc) {$\boldsymbol{\mu}_{\boldsymbol{\beta}}$};
    \node[const, above=of beta, xshift=1cm] (beta_sig) {$\boldsymbol{\Sigma}_{\boldsymbol{\beta}}$};
    \node[const, above=of loc, xshift=1cm] (z) {$\mathbf{z}_{i}$};
    \node[latent, above=of loc, xshift=2cm, yshift=0.5cm] (gamma) {$\boldsymbol{\gamma}$};
    \node[latent, above=of gamma] (tau) {$\tau^{2}$};
    \node[const, above=of gamma, xshift=1cm] (K) {$\mathbf{K}_{\boldsymbol{\gamma}}$};
    \node[const, above=of tau, xshift=-1cm] (a_tau) {$a_{\tau^{2}}$};
    \node[const, above=of tau, xshift=1cm] (b_tau) {$b_{\tau^{2}}$};
    \node[latent, above= of y, xshift=4cm] (scale) {$\sigma$};
    \node[const, above=of scale, xshift=-1cm] (a_sig) {$a_{\sigma}$};
    \node[const, above=of scale, xshift=1cm] (b_sig) {$b_{\sigma}$};

    % Connect the nodes
    \edge [shorten <= 2pt] {loc, scale} {y};
    \edge [shorten >= 2pt, shorten <= 2pt] {x, beta, z, gamma} {loc};
    \edge [shorten <= 2pt] {beta_loc, beta_sig} {beta};
    \edge [shorten <= 2pt] {K} {gamma};
    \edge [shorten <= 2pt] {tau} {gamma};
    \edge [shorten <= 2pt] {a_tau, b_tau} {tau};
    \edge [shorten <= 2pt] {a_sig, b_sig} {scale};

    % Plates
    \plate {y_loc} {(y)(loc)(x)(z)} {$i = 1, \dots, n$};

\end{tikzpicture}
```

Compared to DAGs with only one node type, one can distinguish between different node types in a probabilistic graphical model. Nodes that are random variables are drawn with a circle, while constant (non-random) nodes are drawn without a circle. Constant nodes are either observed data, linear predictor nodes or hyperparameters of the prior distributions. Moreover the observed data and hyperparameters constitute the leaves of the graph. The generality of this framework proves highly beneficial when constructing Bayesian hierarchical (regression) models. It not only offers reusable building blocks for creating a wide variety of probabilistic models in computer programs but also directly implies the sequence of updating the nodes during inference due to the directed nature of the graph. Efficient traversal algorithms can thus be employed to navigate through the DAG, taking into account the interdependencies among the nodes. To derive the traversal order in a DAG, a widely recognized algorithm is the topological sort algorithm [@Manber1989, chap. 7.4].