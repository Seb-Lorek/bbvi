---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Bayesian hierarchical modelling {#sec-bhm}

In Bayesian hierarchical modelling, we explicitly express our assumptions not only concerning the data through the likelihood function but also consider the model parameters as random variables with associated prior probability distributions. For certain hyperparameters of a prior we may even set some hyperprior probability distributions. This means we are fully transparent on the distributional assumptions in our statistical model. Thus we set the joint distribution of the model to a specific factorized form, which in turns implies a hierarchical structure. The hierarchical structure comes from the fact that a conditional distribution dictates the dependencies between random variables and hence the flow of information in our statistical model. This furthermore allows to translate Bayesian hierarchical models into the framework of directed graphical models. In this section we introduce the class of Bayesian semiparametric distributional regression models, being a important subcategory of Bayesian hierarchical models, for which we want to asses the statistical properties of BBVI in comparison to MCMC in simulation studies later on ([Chapter @sec-sim]). Moreover we consider how to integrate this model class into the framework of probabilistic graphical models, also referred to as Bayesian networks.

## Distributional regression {#sec-dist-reg}

Generalized linear models (GLM) unify all regression models that relate a linear predictor to the mean of the response, with a specified link function, for a parametric conditional response distribution [@Fahrmeir2021, p. 623]. A well known model that belongs to the class of GLMs which we already introduced in this thesis is the Bayesian logistic regression model (@sec-binf). Distributional regression goes one step further and potentially relates covariates to further moments of the conditional response distribution [@Fahrmeir2021, chap. 10]. For a conditional normal distributed response it might be of interest to also model the scale (standard deviation) in addition to the location (mean), due to heteroscedastic resiudals when using a location only specification, giving us the well known location-scale regression model^[Due to notational convenience we omit the dependence on the covariates (data).]

$$
\begin{split}
    y_{i} &\ind \mathcal{N}(\mu_{i},  \sigma_{i}^{2}) \\
    \mu_{i} &= \eta_{i, 1} \\
    \sigma_{i} &= \exp(\eta_{i, 2})
\end{split}
$$ {#eq-loc-scale}

The linear predictors $\eta_{i, l}, \ l = 1,2$ contain the effects of the covariates. Such models hence allow the researcher to study the impact of covariates beyond the mean [@Fahrmeir2021, p. 623]. This can be motivated by modelling the response distribution more realistically or by focusing on certain properties of the distribution guided by a specific research question. The thesis only considers Bayesian (semi-)parametric distributional regression, however there is also a large literature on non-parameteric models, being completely distribution free [@Fahrmeir2021, pp. 625]. It is of course also possible to extend the analysis beyond tradtitional location-scale regression by modelling moments such as skewness and kurtosis. To achieve this, one may opt for a more versatile distribution in lieu of the normal distribution. GAMLSS provide a unifying framework for distributional regression, first proposed by @Rigby2005. Here we allow for more flexible response distributions and potentially model any of the parameters that parameterize the distribution [@Fahrmeir2021, p. 641]. Furthermore we can potentially use structured additive forms in the linear predictors. The linear predictors in (@eq-loc-scale) f.e. usually combine fixed and smooth terms such that we obtain the following additive structure 

$$
\eta_{i, l} = \xvec_{i}^{\mathrm{T}} \betavec^{l} + f_{1}^{l}(\nu_{i, 1}) + \ldots + f_{p}^{l}(\nu_{i, p}).
$$ {#eq-lpred}

$\nu_{i,1}, \ldots, \nu_{i, p}$ are the covariates that we model with a non-linear function $f$. Bear in mind that we can include different sets of covariates in each linear predictor $l$. The choice of the function $f$ that models the smooth effects will be dealt with in @sec-semi-para. A general model specification of GAMLSS, in the case of independent responses, is given by

$$
\begin{split}
    y_{i} &\ind p(\vartheta_{i, 1}, \ldots, \vartheta_{i, g}) \\
    \vartheta_{i, l} &= h^{-1}_{l}(\eta_{i, l}), \ l = 1, \ldots, g.
\end{split}
$$

To appropriately respect the parameter space of $\vartheta_{i, l}$, we employ a monotonic inverse link function $h^{-1}_{l}$ [@Fahrmeir2021, p. 645]. In GAMLSS we might even include furhter terms like latent parameters (random-effects) [@Rigby2005, p. 508] into the linear predictor, however in this thesis we will not go beyond the linear predictor specification shown in (@eq-lpred). Traditionally inference in GAMLSS relied either on the RS or CG algorithm [@Rigby1996; @Rigby2005] or MCMC methods in a Bayesian distributional regression setup [@Klein2015]. Modern inference techniques, such as BBVI, may offer intriguing use cases by enabling the expansion of model complexity and scalability to large data sets. In Bayesian semiparametric distributional regression, we naturally augment all parameters with their respective priors, thereby obtaining a hierarchical structure.

## Semiparametric covariate effects {#sec-semi-para}

In the context of Bayesian semiparametric distributional regression we can flexibly combine fixed and smooth effects in the linear predictor. This may be especially admissible when we expect that the relationship between a covariate and reponse is highly non-linear and difficult to model using simple polynomial transformations only. For such scenarios, @Eilers1996 proposed a solution using penalized basis Splines (P-Splines), which allow us to flexibly model the relationship at hand but to maintain a smooth fit at the same time. @Lang2004 extended this idea to a Bayesian approach, which offers an alternative possibility of estimating the coefficients and finding a suitable smoothing parameter. P-splines are especially interesting as they allow to estimate a non-linear and smooth function $f$ in the linear predictor relatively parsimonious compared to alternatives like smoothing splines [@Lang2004, p. 184]. To begin with, the covariate domain is divided into distinct intervals, which are separated by equally spaced knots. The usual choice is to use 20-40 knots and use the (inverse) smoothing parameter to enforce a smooth fit. In a next step, basis functions are placed across the intervals using basis spline (B-spline) basis functions of degree $l$. To obtain a twice differentiable basis function it is common to use a degree of 3, for an in depth definition of the B-spline basis consult @Fahrmeir2021 [pp. 445]. Using a weighted linear combination of $d$ B-spline basis functions obtains the following specification for the smooth effect function

$$
f(\nu_{i}) = \sum_{j=1}^{d} \gamma_{l} B_{j}(\nu_{i}).
$$

The covariate gets evaluated at all basis functions $B_{j}(\nu_{i})$, giving us a design matrix $\Z$, for each smooth covariate effect in the linear predictor, with the following structure

$$
\Z = \begin{bmatrix} 
     B_{1}(\nu_{1}) & \cdots & B_{d}(\nu_{1}) \\
     \vdots & \ddots & \vdots \\
     B_{1}(\nu_{n}) & \cdots & B_{d}(\nu_{n})
     \end{bmatrix}.
$$

To obtain a smooth fit in a Bayesian setting we replace the squared difference penalty from P-spline estimation with appropriate random walk priors for the coefficients, which are the stochastic analogues. Let $\gammavec = \left(\gamma_{1}, \ldots, \gamma_{d}\right)^{\mathrm{T}}$ denote the vector of weighting coefficients for a smooth effect, for which we define a random walk prior of difference order k. If we condition $\gamma_{j}$ on its preceding coefficients and consider a random walk of order $k = 1,2$, we can define the following prior distribution using the Markov property 

$$
\begin{split}
    \gamma_{j} | \gamma_{j-1} &\sim \mathcal{N}(\gamma_{j-1}, \tau^{2}), \ \text{for} \ k = 1 \\
    \gamma_{j} | \gamma_{j-1}, \gamma_{j-2} &\sim \mathcal{N}(2\gamma_{j-1} - \gamma_{j-2}, \tau^{2}), \ \text{for} \ k = 2
\end{split}
$$

Since we usually set k to 1 or 2, we assume non-informative priors for the first and potentially second coefficient 

$$
\begin{split}
    p(\gamma_{1}) &\propto const, \ \text{for} \ k = 1, 2 \\
    p(\gamma_{2}) &\propto const, \ \text{for} \ k = 2.
\end{split}
$$

From this prior specification we can also obtain the joint multivariate prior of $\gammavec$, conditional on the variance parameter $\tau^{2}$

$$
p(\gammavec | \tau^{2}) = (2 \pi)^{-\frac{r \left( \frac{1}{\tau^{2}} \K \right)}{2}} \det \left( \frac{1}{\tau^2}K \right)_{+}^{\frac{1}{2}}\exp \left( -\frac{1}{2} \gammavec^{\mathrm{T}}\frac{1}{\tau^{2}} \K \gammavec \right)
$$

The rank deficient precision matrix $\frac{1}{\tau^{2}} \K$ results from the matrix multiplication $\D^{\mathrm{T}}\D$, where $\D$ denotes the difference matrix for the k-th order differences of the coefficients [@Fahrmeir2021, pp. 455-456]. This distribution is a degenerate multivariate normal distribution, leading to an improper prior. Moreover $\det(\cdot)_{+}$ denotes the pseudo determinant, due to the precision matrix being rank deficient. To complement the prior specification we use the inverse gamma distribution as a hyperprior on the hyperparameter $\tau^{2}$

$$
\tau^{2} \sim \text{InvG}(a,b),
$$

where $a$ and $b$ are hyperparameters that need to be set. @Lang2004 [p. 187] propose to use $a = 1$ and $b = 5\text{e-}3/5\text{e-}4/5\text{e-}5$. The parameter $\tau^{2}$ acts as an inverse smoothing parameter where large values imply flexible function shapes, while small values imply smooth function shapes. Because we set a probability distribution on this parameter we also infer the posterior distribution of this parameter during inference. To ensure the identifiability of all parameters in a additive linear predictor, we need to include centering constraints for the smooth functions $f$. This is achieved by using sum to zero constraints on each smooth function $\sum_{i=1}^{n} f(\nu_{i}) = 0$ [@Wood2017, pp. 175-176]. A common choice is to use the QR-decomposition on the column means of the matrix $\Z_{n \times d}$. From this we obtain a matrix $\T_{d \times d-1}$, being the unitary matrix from the QR-decomposition of $\bar{\zvec}_{d \times 1}$ excluding the first column. This matrix is used to transform covariate design matrix to $\Z_{n \times d} \T_{d \times d-1}$ and the penalty matrix to $\T^{\mathrm{T}}_{d-1 \times d} \K_{d \times d} \T_{d \times d-1}$, such that the linear constraint is satisfied. As a byproduct, we reduce the dimension of the internal parameter $\tilde{\gammavec}_{d-1 \times 1}$ for each smooth effect by 1. We can however recover $\gammavec_{d \times 1}$ easily through the following linear transformation

$$
\gammavec_{d \times 1} = \T_{d \times d-1} \tilde{\gammavec}_{d-1 \times 1},
$$

for further details consult @Wood2017 [p. 211]. At last we can rewrite the linear predictors from (@eq-lpred) in a vectorized form including the sum to zero constraint

$$
\begin{split}
\etavec_{l} &= \X_{l} \betavec_{l} + \Z_{1, l}\gammavec_{1, l} + \ldots + \Z_{p, l} \gammavec_{p, l} \\
&= \X_{l} \betavec_{l} + \Z_{1, l} \T_{1, l} \tilde{\gammavec}_{1, l} + \ldots + \Z_{p, l} \T_{p, l} \tilde{\gammavec}_{p, l}.
\end{split}
$$

Being in a Bayesian setting we supplement the fixed regression coefficients with the following common prior  

$$
\betavec \sim \mathcal{N}(\zerovec, 100 \I),
$$

and each of the parameters $\gammavec_{1,l}, \ldots, \gammavec_{p, l}$ with the multivariate degenerate normal prior conditional on the inverse smoothing parameter, which has an inverse gamma prior with a specification discussed above. Such a set-up could in theory easily be augmented by including interactions between smooth effects and a covariate as in varying coefficient models (VCM) or approximations of two dimensional smooth surface functions [@Lang, p. 188 ff.].

## Directed graphical models {#sec-dgm}

A Bayesian network or probabilistic graphical model is generally a probability distribution for the random variables $\{x_{1}, \ldots, x_{n}\}$ that can the factorized in the following way

$$
p(x_{1}, \ldots, x_{n}) = \prod_{i=1}^{n}p(x_{i}| \text{pa}(x_{i})).
$$

In this context $\text{pa}(x_{i})$ are the parents of the variable $x_{i}$. 

The dependence structure in a Bayesian network lend itself to a representation as a Directed Acyclic Graph (DAG). Where every variable in the joint distribution obtains a node and parents are connnected to childs through errors.

Where we have different node types that define the graph structure. 

Propositional variables as nodes and arrows from parents to children. 

The directed nature of the DAG shows us for example how to generate new data under the statistical model or conduct prediction.
