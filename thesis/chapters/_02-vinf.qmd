---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Variational inference {#sec-vinf}

<!-- Some short (2-3 sentences on variational inference) -->

In VI, we approximate the posterior distribution using a variational distribution from a parametric family $\mathcal{Q}$. Through optimization, we adjust the variational parameters to ensure that the variational distribution closely aligns with the posterior distribution in terms of the KL divergence. This optimized variational distribution subsequently becomes our most accurate approximation to the exact posterior distribution. To gain a comprehensive understanding of VI, we will begin our investigation by examining Bayesian inference within the context of two prevalent hierarchical Bayesian regression models. The first is a simple Bayesian linear regression model and the second is a Bayesian logistic regression model. From this foundation, we will progressively construct the process of VI in this chapter.

## Bayesian inference {#sec-binf}

<!-- Introduce Bayesian inference with two simple examples -->

<!-- Start with the Bayesian linear regression model -->

### Bayesian linear regresion {#sec-bayes-lin-reg}

Consider a Bayesian linear regression model with the response $\yvec \in \mathbb{R}^{n}$, a design matrix $\X \in \mathbb{R}^{n \times k}$ contaning covariates and the parameters $\betavec \in \mathbb{R}^{k}$, $\sigma \in \mathbb{R}_{+}$. The full probabilistic regression model, in terms of its likelihood and prior distributions, can be specified as

$$
\begin{split}
    \yvec | \X, \betavec, \sigma &\sim \mathcal{N}(\X \betavec, \sigma^{2} \I) \\
    \betavec &\sim \mathcal{N}(\muvec_{\betavec}, \Sigmabold_{\betavec}) \\
    \sigma &\sim \text{InvG}(\alpha_{\sigma}, \beta_{\sigma}).
\end{split}
$$

In this context, the response is assumed to be conditionally multivariate normal distributed, given both the data and the parameters. Moreover, its a common practice to employ a multivariate normal distributed prior for the regression coefficients, along with an inverse gamma distributed prior for the scale (standard deviation) such that the parameter space of the scale is respected. A widespread specification of the hyperparameters is for example $\muvec_{\betavec} = 0$, $\Sigmabold_{\betavec} = 100^{2} \I$, $\alpha_{\sigma} = 0.01$ and $\beta_{\sigma} = 0.01$, resulting in weakly informative priors. For the purpose of simplifying the notation, we will group all model parameters into the parameter vector $\thetavec$, where $\thetavec = \left( \betavec^{\mathrm{T}}, \sigma \right)^{\mathrm{T}}$. By applying Bayes' theorem alongside conventional operations, we are able to rearrange the posterior

\begin{align}
    p(\thetavec | \yvec, \X) &= \frac{p(\yvec, \X, \thetavec)}{p(\yvec | \X)} \\
    &= \frac{p(\yvec | \X, \thetavec)p(\thetavec)}{p(\yvec | \X)}Â \\
    &= \frac{p(\yvec | \X, \thetavec)p(\thetavec)}{\int p(\yvec | \X, \thetavec)p(\thetavec) \,d \thetavec} \label{eq-post-evidence} \\
    & \propto p(\yvec | \thetavec, \X) p(\thetavec) \label{eq-post-prop}.
\end{align}

The likelihood^[We adhere to the standard convention of overloading a probability function, wherein their dependencies on their arguments are implicit. We will resort to the formal notation $p_{y}(y)$ only when deemed necessary.] of the model is $p(\yvec | \X, \thetavec)$, while $p(\thetavec)$ denotes the prior. For this model the prior is defined as $p(\thetavec) = p(\betavec | \muvec_{\betavec}, \Sigmabold_{\betavec})p(\sigma | \alpha_{\sigma}, \beta_{\sigma})$. In addition we need the evidence $p(\yvec | \X)$, to calculate the posterior density $p(\thetavec | \yvec, \X)$, which we can obtain from the integral in the denominator of \eqref{eq-post-evidence}. Obtaining the evidence by solving this integral is generally a challenging task. However if the likelihood belongs to the exponential family then a conjugate prior exits, allowing the posterior to follow a well-known distribution in closed form [@Held2014, pp. 179 ff.]. Here the evidence also admits a closed form solution. But when working with multiple parameters, especially in higher-dimensional spaces, its more common that a closed-form solution for the complete posterior density is the exception rather than the rule. Even if each prior is conjugate to the likelihood on its own we are often not able to find a closed-form solution for the complete posterior density. Yet, in such cases, we are able to find closed-form solution for the full conditional density for each parameter. While this holds for $\betavec$, it does not hold true for $\sigma$ within the existing model specification. Furthermore, numerically solving this integral comes with a complexity of $\mathcal{O}(\left(k+1)^{n}\right)$, causing the complexity to increase exponentially as the number of parameters grows. As a result, the evaluation becomes considerably demanding. Therefore, its typical to circumvent the calculation of this integral in Bayesian inference algorithms, by concentrating solely on the joint density of the model which is proportional to the posterior, as shown in \eqref{eq-post-prop}. A multivariate normal prior for $\betavec$, along with a inverse gamma prior for $\sigma^{2}$ (we set a prior for $\sigma$ above), are each conjugate to the Gaussian likelihood. Consequently, closed-form solutions for the full conditionals of the parameters can be derived. This implies that the inference problem can be readily solved using the well-known Gibbs sampler [@Geman1984; @Gelfand1990], where the full conditionals are in the exponential family. Nonetheless, the complete posterior even for this model specification does not adhere to a recognized closed-form.

For predictive purposes, our objective is to determine the posterior predictive density. This density is the conditional density for a new response observation conditional on both the observed data and, in this instance, the covariates at a new specified value, which are assumed to be fixed.

$$
    p(\hat{y} | \hat{\xvec}, \yvec, \X) = \int p(\hat{y}, \thetavec | \hat{\xvec}, \yvec, \X) \,d \thetavec = \int p(\hat{y} | \hat{\xvec}, \thetavec) p(\thetavec | \yvec, \X) \,d \thetavec
$$ {#eq-post-pred}

To obtain the posterior predictive density, we expand the posterior predictive by incorporating the parameters and subsequently integrating them out. In the subsequent step, we disentangle the joint density of predictions and parameters into its constituent parts: the predictive density and the posterior density. This examination shows that the posterior predictive^[In Bayesian statistics it is common to also differentiate between the prior predictive density and the posterior predictive density. The prior predictive density just uses $p(\thetavec)$, instead of $p(\thetavec | \yvec, \X)$ in (@eq-post-pred).] accounts for the additionally uncertainity that comes from the parameters $\thetavec$, integrating these parameters out. This is in contrats to plug-in prediction which uses a single estimate $\hat{\thetavec}$ of the parameters to predict new values, resulting in the predictive density $p(\hat{y} | \hat{\xvec}, \hat{\thetavec})$ [@Held2014, pp. 291 ff.].

The posterior predictive density in (@eq-post-pred) is the density of a compound probability distribution where we integrate out the parameters of the model. If the prior is conjugate to the likelihood, closed form solutions for the posterior predicitve exist [@Held2014, pp. 303 ff.]. But for models that are non-conjugate evaluation of this integral can be challenging as well. Thus numerical methods are commonly employed.

A technique that is both simple and powerful involves utilizing classical Monte Carlo integration. This becomes possible once we possess samples from the posterior, such as those generated through methods like MCMC, or when we possess a fully defined posterior, whether its in closed form or represented through an approximation. The crucial point is our ability to derive samples from this distribution, enabling the application of Monte Carlo integration for approximating the integral, as demonstrated in (@eq-post-pred-mc).

$$
\begin{split}
    p(\hat{y} | \hat{\xvec}, \yvec, \X) &= \int p(\hat{y} | \hat{\xvec}, \thetavec) p(\thetavec | \yvec, \X) \,d \thetavec \\
    &= \text{E}_{p(\thetavec | \yvec, \X)} \left[ p(\hat{y} | \hat{\xvec}, \thetavec) \right] \\
    &\approx \frac{1}{S} \sum_{s=1}^{S} p(\hat{y} | \hat{\xvec}, \thetavec^{s}), \ \thetavec^{s} \sim p(\thetavec | \yvec, \X)
\end{split}
$$ {#eq-post-pred-mc}

This becomes feasible due to the assurance provided by the strong law of large numbers [@Robert2004, pp. 74 ff.] that states

$$
\begin{split}
\text{E}_{p(\thetavec | \yvec, \X)} \left[ p(\hat{y} | \hat{\xvec}, \thetavec) \right] &= \lim_{S \to \infty} \frac{1}{s} \sum_{s=1}^{S} p(\hat{y} | \hat{\xvec}, \thetavec^{s}), \ \thetavec^{s} \sim p(\thetavec | \yvec, \X) .
\end{split}
$$

It is crucial to remember that we now either assumed to have samples from the posterior distribution or possess the ability to sample from the posterior distribution. If we only have an approximate distribution of the posterior, we can also only derive an approximate posterior predictive.

### Bayesian logistic regression {#sec-bayes-logit-reg}

<!-- Show the same with Bayesian logistic regression -->

Another straightforward model that lacks the characteristic of possessing a posterior within the exponential family is Bayesian logistic regression

$$
\begin{split}
    y_{i} | \xvec_{i}, \betavec &\sim \text{Bern} \left( \sigma(\xvec_{i}^{\mathrm{T}} \betavec) \right) \\
    \betavec &\sim \mathcal{N}(\muvec_{\betavec}, \Sigmabold_{\betavec})
\end{split}
$$

The response takes values for $y_{i} \in \{0,1\}$ and follows a conditional Bernoulli distribution. Moreover, the paramter vector $\betavec$ has a multivariate normal distributed prior. The linear predictor $\eta_{i} = \xvec_{i}^{\mathrm{T}} \betavec$ is usually connected to the expected value of the response via the sigmoid function $\text{E}[y_{i} | \xvec_{i}, \betavec] = \pi_{i} = \sigma(\eta_{i}) = \frac{1}{1 + \exp(- \eta_{i})}$. While the linear predictor is defined over $\mathbb{R}$, the mean of the reponse is constrained to $\pi_{i} \in [0,1]$, thus appropriately respecting the paramter space requirements of a probability. Subsequently, the posterior is formulated as follows

$$
\begin{split}
    p(\betavec | \yvec, \X) &= \frac{p(\yvec | \X, \betavec) p(\betavec)}{p(\yvec | \X)} \\
    &= \frac{\prod_{i=1}^{n}p(y_{i} |Â \xvec_{i}, \betavec)p(\betavec)}{p(\yvec | \X)}.
\end{split}
$$

The multiplication of the Bernoulli likelihood and the multivariate normal prior does not facilitate the straightforward closed-form derivation of a posterior density. Moreover obtaining the evidence in the denominator is as usual not feasible. As a result, we require approximate inference methods to attain the posterior. Prediction essentially operates under the same principles as outlined in (@eq-post-pred). Keeping the objectives of Bayesian inference in mind, we can proceed to formulate the optimization criterion that addresses the inference problem in VI.

## Evidence lower bound {#sec-elbo}

The optimization problem of VI is to find the best approximation to the exact posterior, out of the variational family $\mathcal{Q}$, in terms of the following KL divergence

$$
\begin{split}
    q^{*}(\thetavec) &= \argmin_{q(\thetavec) \in \mathcal{Q}} \text{D}_{\text{KL}} \left(q(\thetavec) ||Â p(\thetavec | \yvec, \X) \right) \\
    &= \argmin_{q(\thetavec) \in \mathcal{Q}} \int q(\thetavec) \ln \left( \frac{q(\thetavec)}{p(\thetavec | \yvec, \X)} \right) \,d \thetavec \\
    &= \argmin_{q(\thetavec) \in \mathcal{Q}} \text{E}_{q(\thetavec)} \left[ \ln \left( \frac{q(\thetavec)}{p(\thetavec | \yvec, \X)} \right) \right] .
\end{split}
$$

Parameters of interest are again collected in $\thetavec$, while $\text{D}_{\text{KL}} \left( q(\thetavec) ||Â p(\thetavec | \yvec, \X) \right)$ denotes the KL diveregence [@Kullback1951] between the variational distribution and the posterior. The KL divergence is always $\geq 0$. Upon solving this optimization problem, $q^{*}(\thetavec)$ represents our most accurate approximation to the exact posterior. One should bear in mind that the variational distribution is typically under-parameterized and often falls short in accurately approximating the posterior distribution [@Zhang2017, p. 3]. Hence we are usually not able to reduce the KL divergence below some arbitrary low value. Moreover, the scope of the family $\mathcal{Q}$ defines the complexity of the optimization problem. But while this objective offers valuable theoretical insight, it often remains infeasible to compute due to the requirement for the exact posterior to evaluate it. Consequently, obtaining the evidence for evaluation becomes necessary, which has been shown to be challenging in general.

Nevertheless we can also introduce an alternative criterion that possesses mathematical equivalence, in terms of the optimized variational distribution, but offers the advantage of efficient solvability. We begin by considering the log evidence $\ln(p(\yvec | \X))$ as a foundational point. Our goal is now to show that the log evidence is bounded from below by the so called **e**vidence **l**ower **bo**und (ELBO), which is defined as

$$
\text{ELBO}(\phivec) = \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right].
$$ {#eq-elbo}

In introducing $\phivec$, this vector represents the parameters that govern the variational distribution. Consequently we fix the variational family to a certain parameterized family of distributions. The ELBO, lacking the evidence term, allows for easier evaluation. Through the subsequent derivation, we can demonstrate the ability to decompose the evidence into a sum of the ELBO and the KL divergence.

\begin{align}
    \ln(p(\yvec | \X)) &= \int q(\thetavec | \phivec) \ln(p(\yvec | \X)) \,d \thetavec \\
    &= \int q(\thetavec | \phivec) \ln \left( \frac{p(\yvec |Â \X) p(\thetavec | \yvec, \X)}{p(\thetavec | \yvec, \X)} \right) \,d \thetavec \\
    &= \int q(\thetavec | \phivec) \ln \left( \frac{p(\yvec, \thetavec | \X)}{p(\thetavec | \yvec, \X)} \right) \,d \thetavec \\
    &= \int q(\thetavec | \phivec) \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \frac{q(\thetavec | \phivec)}{p(\thetavec | \yvec, \X)} \right) \,d \thetavec \\
    &= \int q(\thetavec | \phivec) \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \,d \thetavec + \int q(\thetavec | \phivec) \ln \left( \frac{q(\thetavec | \phivec)}{p(\thetavec | \yvec, \X)} \right) \,d \thetavec \\
    &= \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right] + \text{D}_{\text{KL}}(q(\thetavec | \phivec) || p(\thetavec | \yvec, \X)) \label{eq-log-evidence}
\end{align}

Building upon this outcome, we can now establish that the ELBO serves as a lower bound for the log evidence, given that the KL divergence between $q$ and $p$ is always $\geq 0$, i.e. $\text{D}_{\text{KL}}(q || p) \geq 0$ [@Kullback1951]^[We neglect the random variables and parameters of $q$ and $p$ for notational convenience.]. Hence we obtain

$$
\ln(p(\yvec | \X)) \geq \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right].
$$

This fact established the name ELBO. Both quantities are equal iff $q(\thetavec | \phivec) = p(\thetavec | \yvec, \X)$. @Jordan1999 originally derived the ELBO using Jensen's inequality. We are now in a place to bring everything together and demonstrate the equivalence of maximizing the ELBO w.r.t. $q$ and minimizing the KL divergence w.r.t. $q$. Rearranging \eqref{eq-log-evidence} to

$$
\ln(p(\yvec | \X)) - \text{D}_{\text{KL}}(q || p) = \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right],
$$

and taking the arg max w.r.t $\phivec$, as this vector parameterizes the variational distribution $q$, results in

$$
\begin{split}
    \argmax_{\phivec} \ln(p(\yvec | \X)) - \text{D}_{\text{KL}}(q || p) &= \argmax_{\phivec} \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right] \\
    \argmax_{\phivec} - \text{D}_{\text{KL}}(q || p) &= \argmax_{\phivec} \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right] \\
    \argmin_{\phivec} \text{D}_{\text{KL}}(q || p) &= \argmax_{\phivec} \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right].
\end{split}
$$

As the log evidence does not depend on the variational distribution it is a constant in this maximization problem and can thus be omitted. Since maximizing a negative objective is equivalent to minimizing that objective, we can rewrite the argmax of the negative KL divergence as the argmin of the KL divergence. This shows that maximizing the ELBO w.r.t. $\phivec$ is equivalent to minimizing the KL divergence. Hence we make $q$ as similar as possible to the exact posterior in both cases. Thus we can finally state the optimization criterion that is solved in VI

$$
\hat{\phivec} = \argmax_{\phivec} \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right].
$$ {#eq-elbo-opt}

Such that our best approximation of the posterior, after solving this optimization problem, is $q(\thetavec | \hat{\phivec})$. @Blei2017 [pp. 6 ff.] provide a good intuition of the ELBO and the resulting optimal variational density. We can also rewrite the ELBO as

$$
\begin{split}
    \text{ELBO}(\phivec) &= \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec, \thetavec | \X)}{q(\thetavec | \phivec)} \right) \right] \\
    &= \text{E}_{q(\thetavec | \phivec)} \left[ \ln \left( \frac{p(\yvec| \X, \thetavec)p(\thetavec)}{q(\thetavec | \phivec)} \right) \right] \\
    &= \text{E}_{q(\thetavec | \phivec)} \left[ \ln(p(\yvec| \X, \thetavec)) \right] + \text{E}_{q(\thetavec | \phivec)} \left[ \ln(p(\thetavec)) \right] - \text{E}_{q(\thetavec | \phivec)} \left[ \ln(q(\thetavec | \phivec)) \right] \\
    &= \text{E}_{q(\thetavec | \phivec)} \left[ \ln(p(\yvec| \X, \thetavec)) \right] - \text{D}_{\text{KL}} \left( q(\thetavec | \phivec) || p(\thetavec) \right).
\end{split}
$$

This decomposition depicts that maximization of the ELBO will motivate variational densities for $\thetavec$ that explain the data and at the same time are similar to the prior density. Hence we aim to enhance the expected log-likelihood while reducing the KL divergence between the variational distribution and the prior. Thus the ELBO mimics the common balance of likelihood and prior.

Because the ELBO is a lower bound for the evidence, researchers have proposed to also use it for the task of model selection. Nevertheless, theoretical considerations are not supportive of model selection based on a lower bound [@Blei2017, p. 7]. Moreover, the ELBO represents a non-convex objective function. Consequently, optimization can only assure convergence to a local optimum, which is influenced by the initialization [@Blei2017, p. 11].

## Variational family {#sec-vfam}

<!-- Information on the variational family choosen -->

Up until now, we have not delved into the specifics of defining the variational family $\mathcal{Q}$ and consequently the variational distribution $q$. However, we did discuss the crucial point that the complexity of $q$ significantly influences the process of VI. On the one hand, a more intricate family permits enhanced approximations. On the other hand, this heightened complexity makes optimization more difficult. Hence, establishing a versatile $q$ capable of approximating a wide array of probability distribution shapes while effectively handling optimization complexity becomes a commendable objective. In this section we will start with the most simple variational family known as mean-field variational family. For this reason VI utilizing the mean-field variational family is often referred to as mean-field VI.

The mean-field variational family essentially restricts the parameters of the model
in the variational distribution to be independent of each other. Thus every parameter $\theta_{j}$ has its own probability distribution (factor) $q_{j}$ in the variational distribution, parameterized by $\phivec_{j}$, leading to

$$
q(\thetavec | \phivec) = \prod_{j=1}^{J}q_{j}(\theta_{j} |Â \phivec_{j}).
$$

Here, we decomposed $\thetavec$ as $\thetavec = (\theta_{1}, \dots, \theta_{j} , \dots, \theta_{J})^{\mathrm{T}}$ and $\phivec$ as $\phivec = \left( \phivec_{1}^{\mathrm{T}}, \dots, \phivec_{j}^{\mathrm{T}}, \dots, \phivec_{J}^{\mathrm{T}} \right)^{\mathrm{T}}$. The factors $q_{j}$ can take any parametric form, however we generally need to respect the parameter space correctly. Therefore, employing a normal distribution is suitable for unrestricted continuous parameters in $\mathbb{R}$. However, for continuous parameters constrained to $\mathbb{R}_{+}$, we use log-normal or inverse gamma distributions to appropriately respect the parameter space. Discrete (categorical) parameters must be represented by discrete distributions. These consideriations are crucial because the support of the variational distribution must align with the support of the posterior. The support of the posterior distribution is generally unknown. Therefore, one usually assumes that the support of the posterior distribution is equal to that of the prior [@Kucukelbir2016, p. 5]. During optimization of the ELBO each factor of the variational distribution is choosen, and thus its parameters $\phivec_{j}$, to maximize the ELBO.

Even though the mean-field family can fit any marginal density to the parameters, the assumption of independence among all parameters could potentially be overly restrictive. Therefore, there might be a desire to introduce dependencies between parameters that are interdependent. Of course, incorporating dependencies naturally augments the complexity of the optimization problem.

![Mean-field approximation to a bivariate normal distribution, the true posterior in blue and the mean field in red. Based on @Blei2017 [p. 9, figure 1].](assets/plots/plot1.pdf){width=60% fig-scap="Mean-field approximation." #fig-mean-field}

@fig-mean-field exemplifies how the mean-field variational distribution disregards the dependence between the two parameters. Despite correctly identifying the mean, it fails to capture their interdependence. This serves as an illustration that the variational approximation of the posterior can exhibit significant inaccuracy when the variational family is subject to substantial constraints. Furthermore, the marginal variances of the variational distribution are smaller than those of the exact posterior. This outcome is a consequence of utilizing the zero-avoiding KL divergence, denoted as $\text{D}_{\text{KL}}(q || p)$, which endeavors to minimize $q$ whenever $p$ is small. An alternative approach involves employing the density-covering KL divergence, $\text{D}_{\text{KL}}(p || q)$, which seeks to maximize $q$ whenever $p$ is large. This concern becomes particularly pronounced when the posterior assumes multiple modes, while the variational density only accommodates a single mode. Consequently, a branch of research is dedicated to exploring alternative objectives for VI, as discussed in @Bishop2008 [pp. 466 ff.].

We now reconsider the problem of inference in Bayesian linear regression from @sec-bayes-lin-reg. In a first step we could introduce only one factor in the variational distribution for all regression parameters $\betavec$, hence allowing for dependencies between the regression parameters as a simple augmentation, resulting in

$$
q(\thetavec | \phivec) = q_{\betavec}(\betavec | \phivec_{\betavec}Â )q_{\sigma}(\sigma | \phivec_{\sigma}).
$$

The factor for $\betavec$ can be choosen to be a multivariate normal distribution and the factor for $\sigma$ a log-normal distribution. During optimization we would optimize the mean vector and the covariance matrix of the factor $q_{\betavec}$. This would allow to learn the posterior in @fig-mean-field, by providing more structure to the variational distribution, essentially augmenting the mean-field varational family to blocks of parameters, sometimes also referred to as structured mean-field variational inference [@Wainwright2007]. The term structured VI typically refers to variational families that aim to introduce interdependencies among parameters, enhancing the accuracy of the approximation. For further details in this regard, refer to @Hoffman2014.

Upon defining both the variational family and the joint distribution of the model, which comprises likelihood and priors, we have completely defined the ELBO and are ready to employ strategies to solve this optimization problem.

## Coordinate ascent mean-field variational inference {#sec-cavi}

<!-- Short section on CAVI -->

The traditional inference algorithm to solve VI is coordinate ascent variational inference (CAVI) and is closely connected to the Gibbs sampler [@Bishop2008]. The algorithm uses the mean-field variational family over blocks of parameters and iteratively optimizes the ELBO for each factor, while holding the other factors fixed [@Blei2017]. If the model falls within the category of conditionally conjugate models, in which its full conditionals are part of the exponential family, it can be demonstrated that each optimal factor is proportional to

\begin{align}
    q_{j}(\thetavec_{j} |Â \phivec_{j}) &\propto \exp \left\{ \text{E}_{-j} \left[ \ln(p(\thetavec_{j}| \thetavec_{-j}, \yvec, \X)) \right] \right\} \label{eq-factor-cavi}\\
    &\propto \exp \left\{ \text{E}_{-j} \left[ \ln(p(\thetavec_{j}, \thetavec_{-j}, \yvec | \X)) \right] \right\}.
\end{align}

We now also decomposed $\thetavec$ into $\thetavec = \left( \thetavec_{1}^{\mathrm{T}}, \dots, \thetavec_{j}^{\mathrm{T}}, \dots, \thetavec_{J}^{\mathrm{T}} \right)^{\mathrm{T}}$ and $\thetavec_{-j}$ is $\thetavec$ excluding the parameter block $\thetavec_{j}$. $\text{E}_{-j} \left[ \ln(p(\thetavec_{j}| \thetavec_{-j}, \yvec, \X)) \right]$ is the expected log of the full conditional and the expectation is taken over $\thetavec_{-j}$ and thus $\prod_{l \neq j}q_{l}(\thetavec_{l} | \phivec_{l})$. Moreover, the optimal parameters $\phivec_{j}$ of the factor are equal to the expected parameter of the full conditional [@Blei2017, pp. 17]. Hence we found analytical distributions for $\thetavec_{j}$ given the parameters in the other blocks $\thetavec_{-j}$. Typically, we can further reduce the number of parameters we condition on, by focusing solely on those within the Markov blanket of $\thetavec_{j}$. Notably \eqref{eq-factor-cavi} uses the same full conditional as is found in Gibbs sampling [@Geman1984], which explains their similarity. These results allows us, after initialization, to iteratively update the parameters of the optimal factors to progressively maximize the ELBO [@Hoffman2012].

The effectiveness of CAVI hinges upon our ability to derive an analytical expression for the optimal factors $q_{j}$ and thus also for the optimal variational parameters. In general, CAVI requires a multitude of derivations for each optimal factor and the ELBO. Nevertheless, there exist generalization results for models within the exponential family [@Blei2017, pp. 18 ff.]. Naturally, when dealing with models outside the scope of conditional conjugacy, alternative algorithms become necessary for solving the ELBO. Another well-recognized limitation of classical CAVI is its inefficiency in handling large-scale models. The inference algorithm consistently requires to iterate through the entire dataset when the model encompasses observation-specific latent parameters. For large datasets, in combination with potentially complex models, this becomes exceedingly computationally demanding. This is the rationale by @Hoffman2012, who developed SVI for CAVI, which empowers CAVI to efficiently manage sizable datasets.

Given the focus of this thesis on BBVI for hierarchical Bayesian regression models, we refer to the work by @Blei2017, @Bishop2008 and @Hoffman2012 for an in-depth exploration of CAVI. Subsequently, we proceed with more general strategies for tackling the optimization problem.
