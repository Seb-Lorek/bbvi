---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Introduction {#sec-intro}

<!-- Start with a nice introduction -->

The task of statistical inference entails drawing conclusions from a sample about the underlying population, within the framework of a statistical model requiring estimation of its parameters. While frequentist inference has traditionally dominated the field of statistical inference, Bayesian inference has gained significant traction in recent years, even within the realm of machine learning. The focal point of interest in Bayesian inference lies within the posterior distribution, which encompasses all relevant information about the parameters of a statistical model, treating them as random variables. Following the tradition of Bayes' theorem, the posterior represents the conditional distribution of the parameters given the data. The primary objective of Bayesian inference algorithms is to infer this posterior distribution to draw conclusions about the model parameters and, potentially, utilize it for predictive purposes.

It is widely recognized that the primary challenge in obtaining the posterior distribution is the calculation of the normalizing constant, also referred to as the evidence, for models that are non-conjugate [@Held2014, chap. 8, p. 247]. The evidence typically involves a high-dimensional integral, which poses computational inefficiencies in terms of solvability. Thus, approximate Bayesian inference algorithms have to be utilized to infer the posterior distribution. Variational inference (VI) has emerged as a prominent procedure in this context, drawing increasing attention [@Zhang2017].

<!-- Some literature background on VI and gentle introduction -->

VI originally emerged from the field of machine learning as a technique to approximate probability densities [@Jordan1999]. However, its sphere of application has significantly expanded over the years. Today, VI is also widely employed in Bayesian models to approximate posterior distributions that are challenging to compute. For a comprehensive exploration of VI including some exemplary Bayesian models within a literature review, consult @Blei2017. 

VI's theoretical properties are still not as thoroughly explored as those of traditional Bayesian inference techniques, such as Markov Chain Monte Carlo (MCMC) [@Blei2017, p. 2]. The fundamental concept behind MCMC involves establishing a Markov chain that adheres to detailed balance and ergodicity principles. This ensures that the distribution of the samples derived from the chain will ultimately converge to the posterior distribution, the stationary distribution of the Markov chain, over extended iterations [@Metropolis1953; @Hastings1970; @Robert2004]. In stark contrast, the approach in VI revolves around approximating the posterior using a pre-defined variational distribution family $\mathcal{Q}$. This family is optimized with the aim of locating a member that aligns the variational distribution $q$ as closely as possible with the posterior distribution. This degree of closeness is typically quantified through the Kullback-Leibler (KL) divergence. The KL divergence is a divergence measure used to quantify the proximity between two probability distributions [@Kullback1951]. Hence, our usual objective in VI is to minimize this divergence measure w.r.t. $q$.

This distinction already underscores the most significant difference between MCMC and VI methods. The former employs sampling and possesses the property of converging towards the exact posterior distribution through repeated sampling. In contrast, the latter relies on optimizing a criterion, which will be extensively discussed in [Chapter @sec-vinf]. It solely aims to closely approximate the posterior, in terms of the KL divergence, with a simpler variational distribution. The flexibility of the predefined variational family $\mathcal{Q}$ significantly influences the accuracy of the posterior approximation. The greater the constraints imposed on $\mathcal{Q}$, the less accurate the optimized member $q^{*}$ from this family will be, but the utilization of a simpler variational family allows for faster optimization. The enhancement of VI in terms of computational efficiency and scalability compared to MCMC, particularly when dealing with large dataset and complex models, is a major reason why VI is employed for a wide variety of probabilistic models today [@Blei2017, pp. 2 ff.].

It is important to note that VI finds common applications in probabilistic models featuring latent parameters (variables). These latent parameters must be inferred alongside the global parameters. Latent parameters represent observation-specific parameters that capture localized unobserved effects. In statistical terms, such effects are called random effects. Global parameters, on the other hand, remain consistent across individuals, resembling, for example, fixed regression parameters [@Blei2017, pp. 18 ff.]. Research has demonstrated that, relative to runtime, VI significantly outperformes MCMC in obtaining a good posterior approximation, particularly in complex models containing latent parameters [@Blei2017].

<!-- Information on my thesis -->

This thesis explores "black box" variational inference (BBVI) as proposed by @Ranganath2014, in combination with the reparamterization gradient estimator [@Rezende2014]. Developing a VI algorithm typically involves numerous model-specific derivations, which often impedes the exploration of various models for a given problem. This limitation hinders flexibility and ease of experimentation. To address this, BBVI algorithms aim to overcome these obstacles by offering an inference recipe applicable to a diverse range of probabilistic models. By eliminating the necessity for model-specific derivations, these algorithms facilitate broader application across various models [@Ranganath2014, p. 814]. The implementation of the inference algorithm closely follows @Kucukelbir2016 and the variational family $\mathcal{Q}$ is restricted to the mean-field family over blocks of parameters. 

The model class of interest are Bayesian semiparametric distributional regression models. In frequentist statistics such models are called generalized additive models for location, scale and shape (GAMLSS) [@Rigby2005]. These models are commonly employed in classical statistics and cover a wide range of regression models, relating not only the location of the response to covariates, but potentially further moments. Moreover, different response distributions can be considered, in addition to a normal distribution for the response, thus making regression analysis more flexible. In literature Bayesian models are often referred to as hierarchical models due to their structure, which emerges through the specific factorization of the joint distribution. They can be more broadly situated within the context of probabilistic graphical models. The hierarchical structure of these models, in which priors contribute their information to either subsequent priors or ultimately to the likelihood, lends itself as a compelling use case for representing these models as directed acyclic graphs (DAGs). This not only provides an elegant graphical representation of the models but also facilitates the construction of the models, making the process of model building more straightforward. Such models provide us with a visual language for expressing assumptions about data and its underlying structure. The corresponding inference algorithms let us analyze data under those assumptions, inferring the underlying structure that best explains our observations [@Hoffman2012]. In this thesis, we aim at analyzing the properties of BBVI for such models and compare an implemenation of a BBVI algorithm to MCMC inference, through simulation studies. 

For the technical implementation, we use the open source programming language Python [@python3]. The code has been structured in a Python package named `tigerpy`, which can be accessed on github^[The respository can be accessed on github at: <https://github.com/Seb-Lorek/bbvi>.]. For numerical computations and automatic differentation, `tigerpy` relies on the high performance numerical computing library `JAX` [@jax].

<!-- Structure of the thesis -->

The thesis is structured in the following way: [Chapter @sec-vinf] briefly provides an overview of Bayesian inference and introduces VI. Afterwards, we delve into stochastic variational inference (SVI) and BBVI ([Chapter @sec-svinf]), which are the theoretical foundations for the inference algorithm under consideration. In [Chapter @sec-bhm], we introduce the model class of interest, namely Bayesian semiparametric distributional regression models. We continue by examining the implementation of `tigerpy` and its BBVI algorithm. Furthermore, we present the design and results of the simulation studies in [Chapter @sec-sim]. Finally, the thesis draws a conclusion in [Chapter @sec-concl], discusses strenghts and weaknesses of BBVI and points to directions for future research.