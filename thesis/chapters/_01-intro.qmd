---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Introduction {#sec-intro}

<!-- Start with a nice introduction -->

The task of statistical inference entails drawing conclusions about the underlying population from a sample, within the framework of a statistical model requiring estimation of its parameters. While frequentist inference has traditionally dominated the field of statistical inference, Bayesian inference has gained significant traction in recent years, even within the realm of machine learning. The focal point of interest in Bayesian inference lies within the posterior distribution, which encompasses all relevant information about the parameters of a statistical model, treating them as random variables. Following the tradition of Bayes' theorem, the posterior represents the conditional distribution of the parameters given the data. The primary objective of Bayesian inference algorithms is to infer this posterior distribution to reason about the model parameters and, potentially, utilize it for predictive purposes or the task of model choice.

It is widely recognized that the primary challenge in obtaining the posterior distribution is the calculation of the normalizing constant, also referred to as the evidence, for models that are non-conjugate [@Held2014, chap. 8, p. 247]. The evidence typically involves a high-dimensional integral, which poses computational inefficiencies in terms of solvability. Thus approximate Bayesian inference algorithms have to be utilized to infer the posterior distribution. While a range of Bayesian inference algorithms attempt to bypass the direct calculation of the normalizing constant, variational inference (VI) has emerged as a prominent procedure, drawing increasing attention [@Zhang2017].

<!-- Some literature background on VI and gentle introduction -->

VI originally emerged from the field of machine learning as a technique to approximate probability densities [@Jordan1999]. However, its sphere of application has significantly expanded over the years. Today, VI is also widely employed in Bayesian models to approximate posterior distributions that are challenging to compute. For a comprehensive exploration of VI with some exemplary Bayesian models within a literature review, consult @Blei2017. 

VI's theoretical properties are still not as thoroughly explored as those of traditional Bayesian inference techniques, such as Markov Chain Monte Carlo (MCMC) [@Blei2017, p. 2]. The fundamental concept behind MCMC involves establishing a Markov chain that adheres to detailed balance and ergodicity principles. This ensures that the distribution of the samples derived from the chain will ultimately converge to the posterior distribution, the stationary distribution of the Markov chain, over extended iterations [@Metropolis1953; @Hastings1970; @Robert2004]. In stark contrast, the approach in VI revolves around approximating the posterior using a pre-defined variational distribution family $\mathcal{Q}$. This family is optimized with the aim of locating a member that aligns the variational distribution $q$ as closely as possible with the posterior distribution. This degree of closeness is typically quantified through the Kullback-Leibler (KL) divergence. The KL divergence is a divergence measure used to quantify the proximity between two probability distributions [@Kullback1951]. Hence, our usual objective in VI is to minimize this divergence measure w.r.t. $q$.

This distinction already underscores the most significant contrast between MCMC and VI methods. The former employs sampling and possesses the property of converging towards the exact posterior distribution through repeated sampling. In contrast, the latter relies on optimizing a criterion, which will be extensively discussed in [Chapter @sec-vinf]. It solely aims to closely approximate the posterior, in terms of the KL divergence, with a simpler variational distribution. The flexibility of the predefined variational family $\mathcal{Q}$ significantly influences the accuracy of the posterior approximation. The greater the constraints imposed on $\mathcal{Q}$, the less accurate the optimized member $q^{*}$ from this family will be, but the utilization of a simpler variational family, allows for faster optimization. The enhancement of VI in terms of computational efficiency and scalability compared to MCMC, particularly when dealing with large dataset and complex models, is a major reason why VI is employed for a wide variety of probabilistic models today [@Blei2017, pp. 2 ff.].

It is important to note that VI finds common applications in probabilistic models featuring latent parameters (variables). These latent parameters must be inferred alongside the global parameters. Latent parameters represent observation-specific parameters that capture localized unobserved effects. In statistical terms, such effects are called random effects. Global parameters, on the other hand, remain consistent across individuals, resembling for example fixed regression parameters [@Blei2017, pp. 18 ff.]. Research has demonstrated that VI significantly outperforms MCMC in obtaining superior parameter estimates notably quicker, particularly in complex models containing latent parameters [@Blei2017].

<!-- Information on my thesis -->

This thesis explores "black box" variational inference (BBVI) as proposed by in @Ranganath2014, in combination with the reparamterization gradient estimator [@Rezende2014]. Developing a variational inference algorithm typically involves numerous model-specific derivations, which often impedes the exploration of various models for a given problem. This limitation hinders flexibility and ease of experimentation. To address this, "Black box" variational inference algorithms aim to overcome these obstacles by offering an inference recipe applicable to a diverse range of probabilistic models. By eliminating the necessity for model-specific derivations, these algorithms facilitate broader application across various models [@Ranganath2014, p. 814]. The implementation of the inference algorithm closely follows @Kucukelbir2016 and the variational family $\mathcal{Q}$ is restricted to the mean-field family over blocks of parameters. 

The model class of interest are Bayesian semiparametric distributional regression models. Such models are in frequentist statistics called generalized additive models for location, scale and shape (GAMLSS) and were introduced by @Rigby2005. These models are widely employed in classical statistics and cover a wide range of regression models, relating not only the location of the response to covariates, but potentially further moments. Moreover different response distributions can be considered, assuming not only a normal distribution for the response, thus making regression analysis more flexible. Bayesian regression models are often in literature referred to as hierarchical models due to their structure, which emerges through the specific factorization of the joint distribution. These models can be more broadly situated within the context of directed graphical models. The hierarchical structure of these models, in which priors contribute their information to either subsequent priors or ultimately to the likelihood, lends itself as a compelling use case for representing these models as directed acyclic graphs (DAGs). This not only provides an elegant graphical representation of the models but also facilitates the construction of the models, making the process of model building more straightforward. Such models are termed as probabilistic graphical models and they provide us with a visual language for expressing assumptions about data and its underlying structure. The corresponding inference algorithms let us analyze data under those assumptions, inferring the underlying structure that best explains our observations [@Hoffman2012]. We aim at analyzing the properties of BBVI for such models and compare an implemenation of a BBVI algorithm to MCMC inference, through simulation studies, in this thesis. 

For the technical implementation we use the open source programming language Python [@python3] and the code has been structured in a Python package named `tigerpy` which can be accessed on github^[The respository can be accessed on github at: <https://github.com/Seb-Lorek/bbvi>.]. 

<!-- Structure of the thesis -->

The thesis is structured in the following way. [Chapter @sec-vinf] briefly provides an overview of Bayesian inference and introduces VI afterwards. After that we delve into stochastic variational inference (SVI) and BBVI ([Chapter @sec-svinf]), which are the theoretical foundations for the inference algorithm under consideration. In [Chapter @sec-bhm] we introduce the model class of interest, namely Bayesian semiparametric distributional regression models. We continue by examining the implementation of `tigerpy` and its BBVI algorithm. Furthermore we present the design and results of the simulation studies in [Chapter @sec-sim]. Finally the thesis draws a conclusion and discusses strenghts and weaknesses of BBVI and its implementation in [Chapter @sec-concl].