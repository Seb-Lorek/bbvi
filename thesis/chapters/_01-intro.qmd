---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Introduction {#sec-intro}

<!-- Start with a nice introduction -->

The task of statistical inference entails drawing conclusions about the underlying population from a sample, within the framework of a statistical model whose parameters need to be estimated. While frequentist inference has traditionally dominated the field of statistical inference, Bayesian inference has gained significant traction, even within the realm of machine learning, in recent years. The focal point of interest in Bayesian inference lies within the posterior distribution, which encompasses all relevant information about the parameters of a statistical model, treating them as random variables. Following the tradition of Bayes' theorem, the posterior represents the conditional distribution of the parameters given the data. The primary objective of Bayesian inference algorithms is to infer this posterior distribution to reason about the model parameters and, potentially, utilize it for predictive purposes or the task of model choice.

 It is widely recognized that the primary challenge in obtaining this posterior distribution is the calculation of the normalizing constant, also referred to as the evidence, for models that are non-conjugate [@Held2014, chap. 8, p. 247]. This evidence typically involves a high-dimensional integral, which poses computational inefficiencies in terms of solvability. Thus approximate Bayesian inference algorithms have to be utilized to infere the posterior distribution. While a range of Bayesian inference algorithms attempt to bypass the direct calculation of the normalizing constant, variational inference (VI) has emerged as a prominent procedure, drawing increasing attention [@Zhang2017].

<!-- Some literature background on VI and gentle introduction -->

VI originally emerged from the field of machine learning as a technique to approximate probability densities [@Jordan1999]. However, its sphere of application has significantly expanded over the years. Today, VI is also widely employed in Bayesian models to approximate posterior distributions that are challenging to compute. For a comprehensive exploration of VI with some exemplary Bayesian models within a literature review, consult @Blei2017. Nevertheless, it's important to emphasize that VI also has significant applications in frequentist inference.

VI's theoretical properties are still not as thoroughly explored as those of traditional Bayesian inference techniques, such as Markov Chain Monte Carlo (MCMC) [@Blei2017, p. 2]. The fundamental concept behind MCMC involves establishing a Markov chain that adheres to detailed balance and ergodicity principles. This ensures that the distribution of the samples derived from the chain will ultimately converge to the posterior distribution, the stationary distribution of the Markov chain, over extended iterations [@Metropolis1953; @Hastings1970; @Robert2004]. In stark contrast, the approach in VI revolves around approximating the posterior using a pre-defined variational distribution family $\mathcal{Q}$. This family is optimized with the aim of locating a member that aligns the variational distribution $q$ as closely as possible with the posterior distribution. This degree of closeness is typically quantified through the Kullback-Leibler (KL) divergence. The KL divergence is a distance measure used to quantify the proximity between two probability distributions [@Kullback1951]. Hence, our usual objective in VI is to minimize this distance measure w.r.t. $q$.

This distinction already underscores the most significant contrast between MCMC and VI methods. The former employs sampling and possesses the property of converging towards the exact posterior distribution through repeated sampling. In contrast, the latter relies on optimizing a criterion, which will be extensively explored in @sec-vinf. It solely aims to closely approximate the posterior, in terms of the KL divergence, with a simpler variational distribution. The flexibility of the predefined variational family $\mathcal{Q}$ significantly influences the accuracy of the posterior approximation. The greater the constraints imposed on $\mathcal{Q}$, the less accurate the optimized member $q^{*}$ from this family will be. But the utilization of a simpler variational family, allows for faster optimization. The enhancement of VI in terms of computational efficiency and scalability compared to MCMC, particularly when dealing with extensive dataset and complex models, is a major reason why VI is today employed for a wide variety of probabilistic models [@Blei2017, pp. 2 ff.].

It's important to note that VI finds common application in probabilistic models featuring latent parameters (variables). These latent parameters must be inferred alongside the global parameters. Latent parameters represent observation-specific parameters that capture localized unobserved effects. In statistical terms, such models are referred to as mixed-effect models. Global parameters, on the other hand, remain consistent across individuals, resembling f.e. regression parameters [@Blei2017, pp. 18]. It has been shown that VI is able to obtain better parameter estimates much faster compared to MCMC for complex models that contain such latent parameters [@Blei2017]. 

<!-- Information on my thesis -->

This thesis explores "black-box" variational inference (BBVI) proposed in @Ranganath2014, in combination with the reparamterization gradient estimator [@Rezende2014]. The implementation of the algorithm closely follows @Kucukelbir2016. The variational family $\mathcal{Q}$ is restricted to the mean-field family over blocks of parameters. The model class of interest are Bayesian semiparametric distributional regression models, sometimes more broadly referred to as hierarchical Bayesian regression models. Such models are in frequentist statistics called generalized additive models for location, scale and shape (GAMLSS) and were introduced by @Rigby2005. These models are widely employed in classical statistics and cover a wide range of regression models. Relating not only the location of the response to covariates, but potentially further moments. Moreover different response distributions can be flexibly considered, assuming not only a normal distribution for the response, thus making regression analysis more accurate. Bayesian regression models are often more broadly reffered to as hierarchical models due to their structure that emerges through the specific factorization of the joint distribution. Hierarchical Bayesian regression models can be more broadly situated within the context of directed graphical models. The hierarchical structure of these models, in which priors contribute their information to either subsequent priors or ultimately to the likelihood, lends itself as a compelling use case for representing these models as directed acyclic graphs (DAGs). This not only provides an elegant graphical representation of the models but also facilitates the construction of the models, making the process of model building more straightforward. The models are termed as probabilistic graphical models and they provide us with a visual language for expressing assumptions about data and its hidden structure. The corresponding inference algorithms let us analyze data under those assumptions, inferring the hidden structure that best explains our observations [@Hoffman2012]. We aim at analyzing the properties of BBVI for such models and compare an implemenation of a BBVI algorithm to MCMC inference in this thesis. 

<!-- Information on packages and technology -->

Technology wise the thesis uses the open source programming language `python` [@python3] and the code has been structured in a "loosely" written `python` package named `tigerpy` which can be accessed on github^[The respository can be accessed on github at: <https://github.com/Seb-Lorek/bbvi>.]. The package comprises a model building library utilizing the networkx package [@networkx] to construct a DAG, offering efficient attribute storage, streamlined traversal of the DAG during the inference algorithm, and visualization features for the DAG. For numerical computations and automatic differentation `tigerpy` relies on the high perfomance numerical computing library `JAX` [@jax]. For MCMC inference, we leverage the `python` package `liesel` [@Riebl2022]. This package offers support for the considered models and adheres to similar paradigms. Notably, it also utilizes `JAX`, making it a good competitor to the implemented inference algorithm.

<!-- Structure of the thesis -->

The thesis is structured in the following way. [Chapter @sec-vinf] briefly provides an overwiev of Bayesian inference and introduces VI afterwards. After that we delve into stochastic variational inference (SVI) and BBVI ([Chapter @sec-svinf]), which are the theoretical foundations for the inference algorithm under consideration. In [Chapter @sec-bhm] we introduce the model class of interest, being Bayesian semiparametric distributional regression models. We continue by examining the implementation of `tigerpy` and it's BBVI algorithm. Furthermore we present the design and results of the simulation studies in [Chapter @sec-sim]. Finally the thesis draws a conclusion and discusses strenghts and weaknesses of BBVI and it's implementation in [Chapter @sec-concl].