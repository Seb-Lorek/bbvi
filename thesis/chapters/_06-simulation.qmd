---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Simulation studies {#sec-sim}

Simulation studies serve as a vital tool for statisticians, allowing for the evaluation of statistical properties such as bias by leveraging knowledge of the underlying data generation process (DGP). Moreover, they enable the examination of method behavior by altering various aspects of the data and/or method, as already exemplified in @sec-elbo-conv. Within this chapter, we will introduce and examine the outcomes of several simulation studies designed to explore various facets of BBVI. During the whole process of designing, conducting and presenting the study we follow the rationale of @Morris2019. In their article, the authors propose a structured approach denoted by the acronym ADEMP, which stands for Aims, Data-generating mechanisms, Estimands and other targets, Methods, and Performance measures [@Morris2019, p. 2074]. For every simulation study, we will provide an overview of all these different aspects.

## Consistency study {#sec-cons-study}

Frequentist inference emphasizes the significance of consistency as a key property of an estimator. Although working within a Bayesian framework, there persists an interest in investigating the asymptotic behavior of the posterior mean. The Bernstein-von Mises theorem assures us that as $n_{\text{obs}}$ approaches infinity, the posterior distribution (for global parameters) approximates a normal distribution around the posterior mean. This theorem acts as a bridge between frequentist and Bayesian inference methodologies. Hence in our first simulation study we use the posterior means of BBVI as point estimates and asses their asymptotics in terms of bias and empirical standard error (EmpSE) over different samples sizes and regression models. The aim of this study is two fold. First we try to shed light on the asymptotics of the posterior means from BBVI over small as well as larger sample sizes. Furthermore, several authors have previously illustrated that VI yields consistent parameter estimates, as $n_{\text{obs}}$ approaches infinity, when considering the posterior means as point estimates [@You2014; @Neville2014]. Hence, we can rely on the behavior exhibited in large sample sizes as an indicator of whether the implementation functions as anticipated.

We will conduct this study for two well known models being the Bayesian linear regression model ($M_{1}$) and the Bayesian logistic regression model ($M_{2}$) (see @sec-binf). For the data-generating mechanism we generate pseudo-random data for one covariate from a uniform distribution, parameterized by its minimum and maximum value. In detail we use $x \sim \mathcal{U}(-3,3)$. After that we genetrate the data for the linear predictors by applying the corresponding parametric functions on the random covariate vector. Finally we obtain samples from the response distribution. For $M_{1}$, this involves sampling noise from a zero-mean normal distribution, parameterized by its scale with $\sigma = 1$, resulting in $\epsilon \sim \mathcal{N}(0,1)$, and adding this noise to the location. On the other hand, for $M_{2}$, we utilize the logit link function to directly sample from a Bernoulli distribution. We employ the following parametric functions for the study:

1. Linear: $f(x) = \beta_{0} + \beta_{1}x$
2. Quadratic $f(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}$

The parameterizations for the corresponding functions are given in Table \ref{tbl-1}.

\begin{table}[htbp]
\centering
\caption{Parameterization of the data-generating functions, for the consistency study.}
    \begin{tabular}{ll}
        \toprule
        & $\betavec$ \\
        \midrule
        Linear & $[1.0, 2.0]^{\mathrm{T}}$ \\
        Quadratic & $[3.0, 0.2, -0.5]^{\mathrm{T}}$ \\
        \bottomrule
    \end{tabular}
\label{tbl-1}
\end{table}

We use the quadratic function for the location of $M_{1}$ and the linear function for the log-odds of $M_{2}$. The choice for the parameterization of the quadratic function was based on assessing plots. Furthermore, the parameterization of the linear function was chosen to achieve a relatively balanced binary response vector for $M_{2}$, where with the current parameterization we obtain $\frac{1}{n_{\text{obs}}}\sum_{i}^{n_{\text{obs}}}y_{i} \approx 0.58$. To asses the convergence over varying samples sizes $n_{\text{obs}}$ we decide for the following five samples sizes: 50, 100, 500, 1000, 5000. This allows us to study the small sample size behavior as well as large sample asymptotics of the method.

Our Estimands in this study are the fixed known coefficients in Table \ref{tbl-1} and additionally $\sigma = 1$ for $M_{1}$. The method under investigation is the BBVI algorithm (see Alogrithm \ref{algo}). We use an "arbitrary" initialization of the variational parameters and set $\muvec_{j} = \zerovec$ for all coefficients in the linear predictor and $\muvec_{j} = \ln(10)$ for the scale ($\sigma$). For the precision we use $\diag(\Lbold_{j}) = 2$ for the scale and $\diag(\Lbold_{j}) = \onevec$ else. Additionally we use a batch-size of 36, 80% of the data for training, 64 samples from the variational distribution, a max epoch size of 250 and a learning rate of 1e-2. Initial investigations showed that using 250 epochs yielded converging ELBOs also for the smaller sample sizes. Furthermore we also use a new seed for each BBVI run as well as for the data-generating mechanism.

In this context, our performance measures^[See @Morris2019 [p. 2086, Table 6] for a definition of common performance measures, including those that we used.] include bias, empirical standard error (EmpSE). Setting an appropriate number of simulation repetitions $n_{\text{sim}}$ in simulation studies is crucial. We aim for Monte Carlo SEs to be less than 0.1, initial investigations show that this is already the case for $n_{\text{sim}}=100$, thus we conservatively use $n_{\text{sim}}=200$. The simulation study yielded an outcome without any errors or missing estimators.

Kernel density plots depicting the posterior means for the parameters of $M_{1}$ can be found in @fig-kdeplot-loc and @fig-kdeplot-scale. Notably, for small sample sizes, a considerable variation is evident in our estimations. Additionally, the distributions for $\beta_{0}$ and $\beta_{2}$ do not center around their true estimands. Nevertheless, as the sample sizes increase, a convergence to the true estimands becomes apparent. Specifically, after a sample size of 500, our estimators appear to be closely distributed around the true value. In the kernel density estimates of the scale $\sigma$, shown in @fig-kdeplot-scale, we observe a large spread for low sample sizes. We even observe some outliers on the right tail. Furthermore the distributions appear to be right skewed for low sample sizes. Again after a sample size of 500 we observe a close alignment around the true estimand.

![Kernel density for the posterior means of the location parameters of $M_{1}$, true parameters given by $\betavec = [3.0, 0.2, -0.5]^{\mathrm{T}}$ (grey dashed line).](assets/plots/plot8.pdf){width=80% fig-scap="Kernel density for the location parameters of $M_{1}$." #fig-kdeplot-loc}

![Kernel density for the posterior means of the scale parameter of $M_{1}$, true parameter given by $\sigma=1.0$ (grey dashed line).](assets/plots/plot9.pdf){width=80% fig-scap="Kernel density for the scale parameter of $M_{1}$." #fig-kdeplot-scale}

Ideally, we would anticipate observing zero bias across various $n_{\text{obs}}$ sizes, accompanied by a reduction in EmpSE for growing $n_{\text{obs}}$ for our performance measure. Table \ref{tbl-2} depicts the performance measures for the parameters of $M_{1}$. Interestingly especially for smaller sample sizes we observe a sizeable bias for $\beta_{0}$ and $\sigma$, but the bias vanishes for growing samples sizes. Eventhough the bias for $\beta_{2}$ is small in size the respective 95\% confidence intervals (CI) do not cover 0 for $n_{\text{obs}}\leq 1000$. For the EmpSE we observe again rahter large values for $\beta_{0}$ and $\sigma$. However all EmpSEs shrink for growing sample sizes.

\begin{table}[htbp]
\centering
\caption{Simulation results for the parameters of $M_{1}$.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} & \multicolumn{5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11} 
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{-0.4819} & \textbf{-0.2319} & \textbf{-0.0520} & \textbf{-0.0162} & 0.0002 & 0.4563 & 0.3046 & 0.1036 & 0.0640 & 0.0344 \\[-4pt]
         & {\tiny(0.0323)} & {\tiny(0.0215)} & {\tiny(0.0073)} & {\tiny(0.0045)} & {\tiny(0.0024)} & {\tiny(0.0229)} & {\tiny(0.0153)} & {\tiny(0.0052)} & {\tiny(0.0032)} & {\tiny(0.0017)} \\
        $\beta_{1}$ & 0.0113 & 0.0016 & 0.0003 & 0.0021 & -0.0007 & 0.0996 & 0.0598 & 0.0300 & 0.0237 & 0.0166 \\[-4pt]
        & {\tiny(0.0070)} & {\tiny(0.0042)} & {\tiny(0.0021)} & {\tiny(0.0017)} & {\tiny(0.0012)} & {\tiny(0.0050)} & {\tiny(0.0030)} & {\tiny(0.0015)} & {\tiny(0.0012)} & {\tiny(0.0008)}\\
        $\beta_{2}$ & \textbf{0.0972} & \textbf{0.0428} & \textbf{0.0095} & \textbf{0.0033} & -0.0002  & 0.0975 & 0.0614 & 0.0264 & 0.0176 & 0.0104 \\[-4pt]
        & {\tiny(0.0069)} & {\tiny(0.0043)} & {\tiny(0.0019)} & {\tiny(0.0012)} & {\tiny(0.0007)} & {\tiny(0.0049)} & {\tiny(0.0031)} & {\tiny(0.0013)} & {\tiny(0.0009)} & {\tiny(0.0005)} \\
        $\sigma$ & \textbf{0.2389} & \textbf{0.0876} & \textbf{0.0115} & -0.0014 & -0.0021 & 0.2851 & 0.1293 & 0.0409 & 0.0300 & 0.0216 \\[-4pt]
        & {\tiny(0.0202)} & {\tiny(0.0091)} & {\tiny(0.0029)} & {\tiny(0.0021)} & {\tiny(0.0015)} & {\tiny(0.0143)} & {\tiny(0.0065)} & {\tiny(0.0021)} & {\tiny(0.0015)} & {\tiny(0.0011)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}% 
}
\label{tbl-2}
\end{table}

For the parameters of $M_{2}$, we also observe a convergence towards the true estimands. When comparing @fig-kdeplot-loc with @fig-kdeplot-logit, the latter exhibits a wider spread. For @fig-kdeplot-logit we also observe that the distributions appear to be slightly right skewed. Even for $n_{\text{obs}}$ the empirical distribution is not centered around the true value.

![Kernel density for the posterior means of the logit parameters of $M_{2}$, true parameters given by $\betavec = [1.0, 2.0]^{\mathrm{T}}$ (grey dashed line).](assets/plots/plot10.pdf){width=80% fig-scap="Kernel density for the logit parameters of $M_{2}$." #fig-kdeplot-logit}

The findings above are validated in Table \ref{tbl-3}. For the paramters of $M_{2}$ we  observe a slight postitive bias that gradually diminishes with a growing sample size. For both parameters we observe several times that 0 is not covered by the respective 95\% CIs. Moreover the bias also seems rather large in magnitude. However, this bias tends to diminish as the sample size increases. Research also showed that logistic regression demonstrates bias with smaller sample sizes when using maximum likelihood estimation [@Nemes2009]. The EmpSEs once more demonstrate a convergence toward zero. But the EmpSEs are larger in Table \ref{tbl-1} compared Table \ref{tbl-2}.

\begin{table}[htbp]
\footnotesize
\centering
\caption{Simulation results for the parameters of $M_{2}$.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} &\multicolumn {5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11}
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{0.1904} & 0.0631 & 0.0044 & \textbf{-0.0481} & 0.0137 & 0.7180 & 0.5568 & 0.2632 & 0.2072 & 0.1103 \\[-4pt]
        & {\tiny(0.0508)} & {\tiny(0.0394)} & {\tiny(0.0186)} & {\tiny(0.0147)} & {\tiny(0.0078)} & {\tiny(0.0360)} & {\tiny(0.0279)} & {\tiny(0.0132)} & {\tiny(0.0104)} & {\tiny(0.0055)} \\
        $\beta_{1}$ & \textbf{0.3578} & \textbf{0.1914} & \textbf{0.0354} & 0.0111 & \textbf{0.0368} & 0.9137 & 0.6532 & 0.2905 & 0.2349 & 0.1167 \\[-4pt]
        & {\tiny(0.0646)} & {\tiny(0.0462)} & {\tiny(0.0205)} & {\tiny(0.0166)} & {\tiny(0.0083)} & {\tiny(0.0458)} & {\tiny(0.0327)} & {\tiny(0.0146)} & {\tiny(0.0118)} & {\tiny(0.0058)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}%
}
\label{tbl-3}
\end{table}

In this study we showed that on the one hand the posterior means of BBVI yield significantly inaccuracies in small sample sizes, but enjoy the property of consistency. Moreover, the results are promising as there is no indication of an implementation error causing systematic errors and, consequently, biased estimates beyond what one would expect from literature. Remarkably, VI seems to accurately estimate the posterior means of parameters in common regression models with quite high accuracy, in medium to large sample sizes.

## Posterior density study I

In our upcoming study, we aim to compare the posterior distributions derived from fitting a smooth curve, with Bayesian P-splines, using BBVI and MCMC. Are the results closely comparable, or does BBVI yield notably different outcomes compared to MCMC?

The data-generating mechanism is given by 

$$
\begin{split}
    y_{i} | x_{i} &\sim \mathcal{N}(f(x_{i}), 1.5^{2}) \\
    f(x_{i}) &= 3.0 + 1.75 \sin(1.5x_{i}) \\
    x_{i} &\sim \mathcal{U}(-10,10), \ i=1, \dots, 1000,
\end{split}
$$

which we denote in the following as data generating process (DGP). For the observation sample size we use $n_{\text{obs}}=1000$. Furthermore, we slightly heightened the frequency and amplitude of the $\sin$ function to augment the complexity of estimating the function. 

The target of the study is to compare the posterior distributions of MCMC and BBVI visually and by using a metric. We estimted 4 MCMC chains and $n_{\text{sim}}=400$
BBVI runs with different seeds. First we compare 4 MCMC chains and 4 randomly selected BBVI runs through kernel density plots. After that we asses the closeness of the posterior distributions by using the Wasserstein distance [@Kantorovich1960]. As all MCMC chains produced rather similar posterior distribution (see @fig-sim2-fixed - @fig-sim2-scale) we selected one at random and compared its samples with samples geneterated by the optimized variational distributions.

To infere the posterior distribution with BBVI we use our implementation (`tigerpy`) and for MCMC we use the software package `liesel` [@Riebl2022]. This package offers support for inference in probabilistic graphical models through MCMC. Notably, it also utilizes `JAX`, making it a good competitor to the implemented BBVI algorithm. For both models we specify a linear predictor for the location with a fixed intercept $\beta_{0}$ and a smooth effect $\gammavec$ through Bayesian P-splines. We opt for 20 knots, a degree of 3 and a random walk order of 2. In BBVI we again don't use a model specific initialization and set $\muvec_{j}=\zerovec$, $\diag(\Lbold_{j}) = 2\onevec$ for the scale parameter ($\sigma$) and $\diag(\Lbold_{j}) = \onevec$ else. This is done to fairly compare BBVI with MCMC in `liesel`, where we also don't provide a model specific initialization. Furthermore we fix the epoch size to 500, the learning rate to 1e-2, the batch-size to 256, the training share to 0.8 and use 64 samples to evaluate the Monte Carlo integral. For the MCMC algorithm we use a Gibbs sampler Kernel for the inverse smoothing parameter $\tau^{2}$ and a No U-Turn sampler (NUTS) Kernel for all other parameters. We use 1000 samples for the burn-in and after that obtain again 1000 samples from the Markov chain.

The DGP is depicted in @fig-sim2-dgp, while the fits resulting from 1 run of BBVI and 1 chain from MCMC, utilizing the posterior means, are presented in @fig-sim2-post-means. Both methods result in surprisingly similar fits and both are able to capture the smooth function well. Even though BBVI only approximates the posterior distribution with a simpler distribution it intersting to observe how well it inferes the posterior means. Overall, BBVI generally presents a slightly inferior fit. Specifically, at the right boundary, BBVI produces a less accurate fit than MCMC. 

::: {layout="[[50,-5,50]]"}

![The DGP, mean visualized by the solid line and 95% confidence band by the dashed line.](assets/plots/plot11.pdf){fig-scap="Plot of the DGP." #fig-sim2-dgp}

![The DGP and the estimated smooth functions of BBVI and MCMC using the posterior mean.](assets/plots/plot12.pdf){fig-scap="Plot of the estimated smooth functions." #fig-sim2-post-means}

:::

Kernel density plots representing the posterior samples of the model parameters of the 4 MCMC chains and 4 randomly slected BBVI runs are available from @fig-sim2-fixed to @fig-sim2-scale. Once more, we notice similar posterior distributions for both inference methods. However, the resulting posterior distributions from BBVI for different runs appear to exhibit a wider degree of variability. Noticeable differences merges for smooth coefficients on the left boundary (see @fig-sim2-smooth, upper left). MCMC appears to estimate the posterior with considerably higher uncertainty compared to BBVI for these coefficients. 

![Kernel density for the posterior samples of the fixed intercept $\beta_{0}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot13.pdf){width=60% fig-scap="Kernel density for the posterior samples of the fixed intercept." #fig-sim2-fixed}

![Kernel density for the posterior samples of selected internal spline coefficients $\tilde{\gammavec}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot14.pdf){width=60% fig-scap="Kernel density for the posterior samples of selected spline coefficients." #fig-sim2-smooth}

Moreover for the inverse smoothing parameter $\tau^{2}$ we also obtain a considerable difference. But this can be easily related to the fact that we use a Gibbs sampler for the parameter with an inverse gamma distribution for the full conditional, while our factor in the variational distribution is a log-normal distribution. The inverse gamma distribution exhibits a more heavy right tail compared to the log-normal distribution, which is clearly depicted in @fig-sim2-tau2. Although the posterior distributions of the scale parameter appear relatively similar for both methods, @fig-sim2-scale reveals a notably broader range of variability in the posterior distributions for BBVI.

![Kernel density for the posterior samples of the inverse smoothing parameter $\tau^{2}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot15.pdf){width=60% fig-scap="Kernel density for the posterior samples of the inverse smoothing parameter." #fig-sim2-tau2}

![Kernel density for the posterior samples of the scale $\sigma$, using 4 randomly slected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot16.pdf){width=60% fig-scap="Kernel density for the posterior samples of the scale parameter." #fig-sim2-scale}

Table \ref{tbl-4} depicts the statistics of the Wasserstein distance between one MCMC chain and the $n_{\text{sim}}=400$ BBVI runs. The posterior distributions of the intercept $\beta_{0}$ and the scale $\sigma$ quite close in both methods. But for the smooth coeffcients $\gammavec$ as well as the inverse smoothing parameter $\tau^{2}$ differences emerge. We observe generally a higher median for $\gammavec$ compared to $\tau^{2}$, but the Wasserstein distances seem to be heavily right skewed for $\tau^{2}$ due to having a larger mean compared to the median. The visualization in @fig-sim2-wd illustrates that due to the non-convex nature of the ELBO objective, we may get stuck in a bad local optimum, marked by inferior approximations compared to MCMC.

\begin{table}[htbp]
\centering
\caption{Simluation results of the Wasserstein distance ($W_{2}$).}
\begin{threeparttable}
    \begin{tabular}{lllll}
        \toprule
         & \multicolumn{4}{c}{Parameters} \\
        \cmidrule(lr){2-5} 
         & $\beta_{0}$ & $\gammavec$ & $\sigma$ & $\tau^{2}$ \\
        \midrule
        Mean & 0.0227 & 1.7563 & 0.0268 & 3.2765 \\[-4pt]
        & {\tiny(0.0014)} & {\tiny(0.0223)} & {\tiny(0.0013)} & {\tiny(0.0482)} \\
        Median & 0.0179 & 1.6479 & 0.0222 & 3.0931 \\[-4pt]
        & {\tiny(0.0017)} & {\tiny(0.0280)} & {\tiny(0.0016)} & {\tiny(0.0604)} \\
        $Q_{0.25}$ & 0.0086 & 1.5489 & 0.0120 & 2.7244 \\[-4pt]
        & {\tiny(0.0019)} & {\tiny(0.0304)} & {\tiny(0.0017)} & {\tiny(0.0657)} \\
        $Q_{0.75}$ & 0.0287 & 1.8033 & 0.0346 & 3.5848 \\[-4pt]
        & {\tiny(0.0019)} & {\tiny(0.0304)} & {\tiny(0.0017)} & {\tiny(0.0657)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize	
        \item Corresponding Monte Carlo SEs are provided below in parentheses; $n_{\text{sim}}=400$.
    \end{tablenotes}
\end{threeparttable}
\label{tbl-4}
\end{table}

![Box plots displaying the Wasserstein distance for the different model parameters.](assets/plots/plot17.pdf){width=80% fig-scap="Box plots of the Wasserstein distance." #fig-sim2-wd}

## Posterior density study II

In our last study we continue to compare the posterior distribution of BBVI and MCMC but try to quantify the closeness to a posterior by using the KL-divergence. 

We will consider a location-scale regression model with structured additive linear predictors. Compromising two covariates 