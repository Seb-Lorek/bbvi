---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Simulation studies {#sec-sim}

Simulation studies serve as a vital tool for statisticians, allowing for the evaluation of statistical properties such as bias by leveraging knowledge of the underlying data generation process (DGP). Moreover, they enable the examination of method behavior by altering various aspects of the data and/or method, as already exemplified in @sec-elbo-conv. Within this chapter, we will introduce and examine the outcomes of several simulation studies designed to explore various facets of BBVI. During the whole process of designing, conducting and presenting the study we follow the rationale of @Morris2019. In their article, the authors propose a structured approach denoted by the acronym ADEMP, which stands for Aims, Data-generating mechanisms, Estimands and other targets, Methods, and Performance measures [@Morris2019, p. 2074]. For every simulation study, we will provide an overview of all these different aspects.

## Consistency study {#sec-cons-study}

Frequentist inference emphasizes the significance of consistency as a key property of an estimator. Although working within a Bayesian framework, there persists an interest in investigating the asymptotic behavior of the posterior mean. The Bernstein-von Mises theorem [@Vaart2000, chap. 10.2] assures us that as $n_{\text{obs}}$ approaches infinity, the posterior distribution (for global parameters) approximates a normal distribution around the posterior mean. This theorem acts as a bridge between frequentist and Bayesian inference. Hence in our first simulation study we use the posterior means of BBVI as point estimates and asses their asymptotics in terms of bias and empirical standard error (EmpSE) over different samples sizes and regression models. The objectives of this study are multifaceted. First, we try to shed light on the asymptotics of the posterior means from BBVI over small as well as larger sample sizes. Furthermore, several authors have previously illustrated that VI yields consistent parameter estimates, as $n_{\text{obs}}$ approaches infinity, when considering the posterior means as point estimates [@You2014; @Neville2014]. Hence, we can rely on the behavior exhibited in large sample sizes as an indicator of whether the implementation functions as anticipated. Lastly, we also conducted the simulation study with MCMC to asses the relative performance of BBVI. This might enable us to identify circumstances where the application of BBVI is safe and reliable, especially concerning the sample size.

We will conduct this study for two well known models being the Bayesian linear regression model ($M_{1}$) and the Bayesian logistic regression model ($M_{2}$) (see @sec-binf). For the data-generating mechanism we generate pseudo-random data for one covariate from a uniform distribution, parameterized by its minimum and maximum value. In detail we use $x \sim \mathcal{U}(-3,3)$. After that we genetrate the data for the linear predictors by applying the corresponding parametric functions on the random covariate vector. Finally we obtain samples from the response distribution. For $M_{1}$, this involves sampling noise from a zero-mean normal distribution, parameterized by its scale with $\sigma = 1$, resulting in $\epsilon \sim \mathcal{N}(0,1)$, and adding this noise to the location. On the other hand, for $M_{2}$, we utilize the logit link function to directly sample from a Bernoulli distribution. We employ the following parametric functions for the study:

1. Linear: $f(x) = \beta_{0} + \beta_{1}x$
2. Quadratic $f(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}$

The parameterizations for the corresponding functions are given in Table \ref{tbl-1}.

\begin{table}[htbp]
\centering
\caption{Parameterization of the data-generating functions, for the consistency study.}
    \begin{tabular}{ll}
        \toprule
        & $\betavec$ \\
        \midrule
        Linear & $[1.0, 2.0]^{\mathrm{T}}$ \\
        Quadratic & $[3.0, 0.2, -0.5]^{\mathrm{T}}$. \\
        \bottomrule
    \end{tabular}
\label{tbl-1}
\end{table}

We use the quadratic function for the location of $M_{1}$ and the linear function for the log-odds of $M_{2}$. The choice for the parameterization of the quadratic function was based on assessing plots. Furthermore, the parameterization of the linear function was chosen to achieve a relatively balanced binary response vector for $M_{2}$, where we obtain with the current parameterization $\frac{1}{n_{\text{obs}}}\sum_{i}^{n_{\text{obs}}}y_{i} \approx 0.58$. To asses the convergence over varying samples sizes $n_{\text{obs}}$ we decide for the following five samples sizes: 50, 100, 500, 1000, 5000. This allows us to study the small sample size behavior as well as large sample asymptotics of the method.

Our Estimands in this study are the fixed known coefficients in Table \ref{tbl-1} and additionally $\sigma = 1$ for $M_{1}$. The method under investigation is the BBVI algorithm (see Alogrithm \ref{algo}) implemented in `tigerpy`. Furthermore, for evaluating the comparative performance, we employ the No U-turn sampler (NUTS) within MCMC, for all model parameters, in the software package `liesel` [@Riebl2022]. We use an "arbitrary" initialization of the variational parameters and set $\muvec_{j} = \zerovec$ for all coefficients in the linear predictor and $\muvec_{j} = \ln(10)$ for the scale ($\sigma$). For the precision we use $\diag(\Lbold_{j}) = 2$ for the scale and $\diag(\Lbold_{j}) = \onevec$ else. Additionally we use a batch-size of 36, 80% of the data for training, 64 samples from the variational distribution, a max epoch size of 250 and a learning rate of 1e-2. Initial investigations showed that using 250 epochs yielded converging ELBOs also for the smaller sample sizes. Furthermore we also use a new seed for each BBVI run as well as for the data-generating mechanism. For MCMC we decide to use a burn-in of 1000 samples and then obtain 1000 samples from the Markov chain. 

In this context, our performance measures^[See @Morris2019 [p. 2086, Table 6] for a definition of common performance measures, including those that we used.] include bias, empirical standard error (EmpSE). Setting an appropriate number of simulation repetitions $n_{\text{sim}}$ in simulation studies is crucial. We aim for Monte Carlo SEs to be less than 0.1, initial investigations show that this is already the case for $n_{\text{sim}}=100$, thus we conservatively use $n_{\text{sim}}=200$. The simulation study yielded an outcome without any errors or missing estimators. All result for BBVI are included in this Section, while the results for MCMC are included in [Appendix @sec-mcmc-cons].

Kernel density plots depicting the posterior means for the parameters of $M_{1}$ can be found in @fig-kdeplot-loc and @fig-kdeplot-scale. Notably, for small sample sizes, a considerable variation is evident in our estimations. Additionally, the distributions for $\beta_{0}$ and $\beta_{2}$ do not center around their true estimands. Nevertheless, as the sample sizes increase, a convergence to the true estimands becomes apparent. Specifically, after a sample size of 500, our estimators appear to be closely distributed around the true value. In the kernel density estimates of the scale $\sigma$, shown in @fig-kdeplot-scale, we observe a large spread for low sample sizes. Furthermore, the distributions appears to be prominently right skewed for low sample sizes. Again, after a sample size of 500 we observe a close alignment around the true estimand.

![Kernel density for the posterior means of the location parameters of $M_{1}$, true parameters given by $\betavec = [3.0, 0.2, -0.5]^{\mathrm{T}}$ (grey dashed line).](assets/plots/plot8.pdf){width=80% fig-scap="Kernel density for the location parameters of $M_{1}$." #fig-kdeplot-loc}

![Kernel density for the posterior means of the scale parameter of $M_{1}$, true parameter given by $\sigma=1.0$ (grey dashed line).](assets/plots/plot9.pdf){width=80% fig-scap="Kernel density for the scale parameter of $M_{1}$." #fig-kdeplot-scale}

Ideally, we would anticipate observing zero bias across various $n_{\text{obs}}$ sizes, accompanied by a reduction in EmpSE for growing $n_{\text{obs}}$ for our performance measure. Table \ref{tbl-2} depicts the performance measures for the parameters of $M_{1}$. Interestingly especially for smaller sample sizes we observe a sizeable bias for $\beta_{0}$ and $\sigma$, but the bias vanishes in size for growing samples sizes. Notably we still observe a significant bias for $\sigma$ with $n_{\text{obs}}=5000$. Eventhough the bias for $\beta_{2}$ is small in size the respective 95\% confidence intervals (CI)s do not cover 0 for $n_{\text{obs}}\leq 1000$. For the EmpSE we observe rather large values for $\beta_{0}$ and $\sigma$. However all EmpSEs shrink for growing sample sizes.

\begin{table}[htbp]
\centering
\caption{Simulation results for the parameters of $M_{1}$, using BBVI.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} & \multicolumn{5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11} 
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{-0.4514} & \textbf{-0.2191} & \textbf{-0.0428} & -0.0068 & 0.0027 & 0.4303 & 0.2607 & 0.0950 & 0.0604 & 0.0370 \\[-4pt]
         & {\tiny(0.0304)} & {\tiny(0.0184)} & {\tiny(0.0067)} & {\tiny(0.0043)} & {\tiny(0.0026)} & {\tiny(0.0216)} & {\tiny(0.0131)} & {\tiny(0.0048)} & {\tiny(0.0030)} & {\tiny(0.0019)} \\
        $\beta_{1}$ & 0.0118 & 0.0000 & 0.0006 & -0.0009 & -0.0010 & 0.0918 & 0.0586 & 0.0302 & 0.0240 & 0.0183 \\[-4pt]
        & {\tiny(0.0065)} & {\tiny(0.0041)} & {\tiny(0.0021)} & {\tiny(0.0017)} & {\tiny(0.0013)} & {\tiny(0.0046)} & {\tiny(0.0029)} & {\tiny(0.0015)} & {\tiny(0.0012)} & {\tiny(0.0009)}\\
        $\beta_{2}$ & \textbf{0.0907} & \textbf{0.0381} & \textbf{0.0084} & \textbf{0.0029} & -0.0001 & 0.0922 & 0.0571 & 0.0262 & 0.0176 & 0.0107 \\[-4pt]
        & {\tiny(0.0065)} & {\tiny(0.0040)} & {\tiny(0.0019)} & {\tiny(0.0012)} & {\tiny(0.0008)} & {\tiny(0.0046)} & {\tiny(0.0029)} & {\tiny(0.0013)} & {\tiny(0.0009)} & {\tiny(0.0005)} \\
        $\sigma$ & \textbf{0.2207} & \textbf{0.0821} & \textbf{0.0065} & -0.0037 & \textbf{-0.0042} & 0.2939 & 0.1011 & 0.0371 & 0.0299 & 0.0208 \\[-4pt]
        & {\tiny(0.0208)} & {\tiny(0.0072)} & {\tiny(0.0026)} & {\tiny(0.0021)} & {\tiny(0.0015)} & {\tiny(0.0147)} & {\tiny(0.0051)} & {\tiny(0.0019)} & {\tiny(0.0015)} & {\tiny(0.0010)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}% 
}
\label{tbl-2}
\end{table}

For the parameters of $M_{2}$, we also observe a convergence towards the true estimands. When comparing @fig-kdeplot-loc with @fig-kdeplot-logit, the latter exhibits a wider spread. For @fig-kdeplot-logit we furthermore observe, that the distributions appear to be slightly right skewed. Even for large $n_{\text{obs}}$ the empirical distribution is not centered around the true value.

![Kernel density for the posterior means of the logit parameters of $M_{2}$, true parameters given by $\betavec = [1.0, 2.0]^{\mathrm{T}}$ (grey dashed line).](assets/plots/plot10.pdf){width=80% fig-scap="Kernel density for the logit parameters of $M_{2}$." #fig-kdeplot-logit}

The findings above are validated in Table \ref{tbl-3}. We detect a considerable positive bias in the parameters of $M_{2}$, which gradually decreases as the sample size increases. For both parameters we observe several times that 0 is not covered by the respective 95\% CIs, even for larger sample sizes. Research generally showed that logistic regression demonstrates bias in smaller sample sizes when using maximum likelihood estimation [@Nemes2009], which partly helps in explaining the observed results. The EmpSEs once more demonstrate a convergence toward zero. But the EmpSEs in Table \ref{tbl-1} are larger compared Table \ref{tbl-2}.

\begin{table}[htbp]
\footnotesize
\centering
\caption{Simulation results for the parameters of $M_{2}$, using BBVI.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} &\multicolumn {5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11}
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{0.1774} & 0.0663 & 0.0042 & -0.0231 & \textbf{0.0179} & 0.7487 & 0.5414 & 0.2567 & 0.2042 & 0.1139 \\[-4pt]
        & {\tiny(0.0529)} & {\tiny(0.0383)} & {\tiny(0.0182)} & {\tiny(0.0144)} & {\tiny(0.0081)} & {\tiny(0.0375)} & {\tiny(0.0271)} & {\tiny(0.0129)} & {\tiny(0.0102)} & {\tiny(0.0057)} \\
        $\beta_{1}$ & \textbf{0.3617} & \textbf{0.1845} & 0.0322 & \textbf{0.0388} & \textbf{0.0488} & 0.9235 & 0.6648 & 0.2996 & 0.2129 & 0.1123 \\[-4pt]
        & {\tiny(0.0653)} & {\tiny(0.0470)} & {\tiny(0.0212)} & {\tiny(0.0151)} & {\tiny(0.0079)} & {\tiny(0.0463)} & {\tiny(0.0333)} & {\tiny(0.0150)} & {\tiny(0.0107)} & {\tiny(0.0056)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}%
}
\label{tbl-3}
\end{table}

Finally, we compare the performance measures obtained from BBVI with those derived from MCMC. For model $M_{1}$	​, we observe that MCMC produces unbiased estimates, even for low sample sizes (refer to Table \ref{tbl-5}). Additionally, all biases are relatively small in magnitude, a contrast to our findings for BBVI. Moreover, the EmpSEs from BBVI are generally larger than those obtained from MCMC. When comparing both performance measures, BBVI seems to yield similar results for $n_{\text{obs}} \geq 500$.

The results for $M_{2}$ using MCMC are depicted in Table \ref{tbl-6}. We find that MCMC also obtains sizeable significant biases for low to medium samples sizes. However, the biases vanishes for $n_{\text{obs}} \geq 1000$. Comparing the EmpSEs we find that MCMC obtains larger EmpSEs for low samples sizes. In general, it seems that MCMC exhibits faster and more consistent convergence compared to BBVI.

In this study, we demonstrated that while the posterior means of BBVI exhibit substantial inaccuracies in small sample sizes, they possess the property of consistency. However, even with larger sample sizes, we sometimes observed a persistent significant bias, albeit diminished in size. Moreover, we find that with a sample size of 500, BBVI produces comparable results to MCMC. Interestingly, in logistic regression with small sample sizes, BBVI demonstrates a narrower spread of posterior mean estimates compared to MCMC. Finally, there is no indication of an implementation error causing systematic errors and, consequently, biased estimates beyond what one would expect from literature. Remarkably, VI seems to accurately estimate the posterior means of parameters in common regression models with relatively high accuracy, in medium to large sample sizes.

## Posterior density study I

In our upcoming study, we aim to compare the posterior distributions derived from fitting a smooth curve, with Bayesian P-splines, using BBVI and MCMC. Are the results closely comparable, or does BBVI yield notably different outcomes compared to MCMC?

The data-generating mechanism is given by 

$$
\begin{split}
    y_{i} | x_{i} &\sim \mathcal{N}(f(x_{i}), 1.5^{2}) \\
    f(x_{i}) &= 3.0 + 1.75 \sin(1.5x_{i}) \\
    x_{i} &\sim \mathcal{U}(-10,10), \ i=1, \dots, 1000,
\end{split}
$$

which we denote in the following as data generating process (DGP). For the observation sample size we use $n_{\text{obs}}=1000$. Furthermore, we slightly heightened the frequency and amplitude of the $\sin$ function to augment the complexity of estimating the function. 

The target of the study is to compare the posterior distributions of MCMC and BBVI visually and by using a metric. We estimted 4 MCMC chains and $n_{\text{sim}}=400$
BBVI runs with different seeds. First we compare 4 MCMC chains and 4 randomly selected BBVI runs through kernel density plots. After that we asses the closeness of the posterior distributions by using the Wasserstein distance [@Kantorovich1960]. As all MCMC chains produced rather similar posterior distribution (see @fig-sim2-fixed - @fig-sim2-scale) we selected one at random and compared its samples with samples geneterated by the optimized variational distributions.

To infere the posterior distribution with BBVI we use our implementation (`tigerpy`) and for MCMC we use the software package `liesel` [@Riebl2022]. This package offers support for inference in probabilistic graphical models through MCMC. Notably, it also utilizes `JAX`, making it a good competitor to the implemented BBVI algorithm. For both models we specify a linear predictor for the location with a fixed intercept $\beta_{0}$ and a smooth effect $\gammavec$ through Bayesian P-splines. We opt for 20 knots, a degree of 3 and a random walk order of 2. In BBVI we again don't use a model specific initialization and set $\muvec_{j}=\zerovec$, $\diag(\Lbold_{j}) = 2\onevec$ for the scale parameter ($\sigma$) and $\diag(\Lbold_{j}) = \onevec$ else. This is done to fairly compare BBVI with MCMC in `liesel`, where we also don't provide a model specific initialization. Furthermore we fix the epoch size to 500, the learning rate to 1e-2, the batch-size to 256, the training share to 0.8 and use 64 samples to evaluate the Monte Carlo integral. For the MCMC algorithm we use a Gibbs sampler Kernel for the inverse smoothing parameter $\tau^{2}$ and a No U-Turn sampler (NUTS) Kernel for all other parameters. We use 1000 samples for the burn-in and after that obtain again 1000 samples from the Markov chain.

The DGP is depicted in @fig-sim2-dgp, while the fits resulting from 1 run of BBVI and 1 chain from MCMC, utilizing the posterior means, are presented in @fig-sim2-post-means. Both methods result in surprisingly similar fits and both are able to capture the smooth function well. Even though BBVI only approximates the posterior distribution with a simpler distribution it intersting to observe how well it inferes the posterior means. Overall, BBVI generally presents a slightly inferior fit. Specifically, at the right boundary, BBVI produces a less accurate fit than MCMC. 

::: {layout="[[50,-5,50]]"}

![The DGP, mean visualized by the solid line and 95% confidence band by the dashed line.](assets/plots/plot11.pdf){fig-scap="Plot of the DGP." #fig-sim2-dgp}

![The DGP and the estimated smooth functions of BBVI and MCMC using the posterior mean.](assets/plots/plot12.pdf){fig-scap="Plot of the estimated smooth functions." #fig-sim2-post-means}

:::

Kernel density plots representing the posterior samples of the model parameters of the 4 MCMC chains and 4 randomly slected BBVI runs are available from @fig-sim2-fixed to @fig-sim2-scale. Once more, we notice similar posterior distributions for both inference methods. However, the resulting posterior distributions from BBVI for different runs appear to exhibit a wider degree of variability. Noticeable differences merges for smooth coefficients on the left boundary (see @fig-sim2-smooth, upper left). MCMC appears to estimate the posterior with considerably higher uncertainty compared to BBVI for these coefficients. 

![Kernel density for the posterior samples of the fixed intercept $\beta_{0}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot13.pdf){width=60% fig-scap="Kernel density for the posterior samples of the fixed intercept." #fig-sim2-fixed}

![Kernel density for the posterior samples of selected internal spline coefficients $\tilde{\gammavec}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot14.pdf){width=60% fig-scap="Kernel density for the posterior samples of selected spline coefficients." #fig-sim2-smooth}

Moreover for the inverse smoothing parameter $\tau^{2}$ we also obtain a considerable difference. But this can be easily related to the fact that we use a Gibbs sampler for the parameter with an inverse gamma distribution for the full conditional, while our factor in the variational distribution is a log-normal distribution. The inverse gamma distribution exhibits a more heavy right tail compared to the log-normal distribution, which is clearly depicted in @fig-sim2-tau2. Although the posterior distributions of the scale parameter appear relatively similar for both methods, @fig-sim2-scale reveals a notably broader range of variability in the posterior distributions for BBVI.

![Kernel density for the posterior samples of the inverse smoothing parameter $\tau^{2}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot15.pdf){width=60% fig-scap="Kernel density for the posterior samples of the inverse smoothing parameter." #fig-sim2-tau2}

![Kernel density for the posterior samples of the scale $\sigma$, using 4 randomly slected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot16.pdf){width=60% fig-scap="Kernel density for the posterior samples of the scale parameter." #fig-sim2-scale}

Table \ref{tbl-4} depicts the statistics of the Wasserstein distance between one MCMC chain and the $n_{\text{sim}}=400$ BBVI runs. The posterior distributions of the intercept $\beta_{0}$ and the scale $\sigma$ quite close in both methods. But for the smooth coeffcients $\gammavec$ as well as the inverse smoothing parameter $\tau^{2}$ differences emerge. We observe generally a higher median for $\gammavec$ compared to $\tau^{2}$, but the Wasserstein distances seem to be heavily right skewed for $\tau^{2}$ due to having a larger mean compared to the median. The visualization in @fig-sim2-wd illustrates that due to the non-convex nature of the ELBO objective, we may get stuck in a bad local optimum, marked by inferior approximations compared to MCMC.

\begin{table}[htbp]
\centering
\caption{Simluation results of the Wasserstein distance ($W_{2}$).}
\begin{threeparttable}
    \begin{tabular}{lllll}
        \toprule
         & \multicolumn{4}{c}{Parameters} \\
        \cmidrule(lr){2-5} 
         & $\beta_{0}$ & $\gammavec$ & $\sigma$ & $\tau^{2}$ \\
        \midrule
        Mean & 0.0227 & 1.7563 & 0.0268 & 3.2765 \\[-4pt]
        & {\tiny(0.0014)} & {\tiny(0.0223)} & {\tiny(0.0013)} & {\tiny(0.0482)} \\
        Median & 0.0179 & 1.6479 & 0.0222 & 3.0931 \\[-4pt]
        & {\tiny(0.0017)} & {\tiny(0.0280)} & {\tiny(0.0016)} & {\tiny(0.0604)} \\
        $Q_{0.25}$ & 0.0086 & 1.5489 & 0.0120 & 2.7244 \\[-4pt]
        & {\tiny(0.0019)} & {\tiny(0.0304)} & {\tiny(0.0017)} & {\tiny(0.0657)} \\
        $Q_{0.75}$ & 0.0287 & 1.8033 & 0.0346 & 3.5848 \\[-4pt]
        & {\tiny(0.0019)} & {\tiny(0.0304)} & {\tiny(0.0017)} & {\tiny(0.0657)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize	
        \item Corresponding Monte Carlo SEs are provided below in parentheses; $n_{\text{sim}}=400$.
    \end{tablenotes}
\end{threeparttable}
\label{tbl-4}
\end{table}

![Box plots displaying the Wasserstein distance for the different model parameters.](assets/plots/plot17.pdf){width=80% fig-scap="Box plots of the Wasserstein distance." #fig-sim2-wd}

## Posterior density study II

In our last study we continue to compare the posterior distribution of BBVI and MCMC but try to quantify the closeness to a posterior by using the KL-divergence. 

We will consider a location-scale regression model with structured additive linear predictors. Compromising two covariates 