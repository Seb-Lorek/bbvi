---
bibliography: ["bib/references.bib", "bib/packages.bib"]
---

# Simulation studies {#sec-sim}

Simulation studies serve as a vital tool for statisticians, allowing the evaluation of statistical properties such as bias by leveraging knowledge of the underlying data generation process (DGP). Moreover, they enable the examination of method behavior by altering various aspects of the data and/or method, as already exemplified in @sec-elbo-conv. Within this chapter, we will introduce and examine the outcomes of several simulation studies designed to explore various facets of BBVI^[All studies were run on a MacBook Pro (Retina 13 intch, 2015) with a 2.7 GHz Dual-Core Intel Core i5 and 8 GB 1867 MHz DDR3 ram.]. Throughout the process of designing, conducting and presenting the study, we follow the rationale of @Morris2019. In their article, the authors propose a structured approach denoted by the acronym ADEMP, which stands for Aims, Data-generating mechanisms, Estimands and other targets, Methods, and Performance measures [@Morris2019, p. 2074]. For every simulation study, we will provide an overview of all these different aspects.

## Consistency study {#sec-cons-study}

Frequentist inference emphasizes the significance of consistency as a key property of an estimator. Although working within a Bayesian framework, there persists an interest in investigating the asymptotic behavior of the posterior mean. The Bernstein-von Mises theorem [@Vaart2000, chap. 10.2] assures us that, as $n_{\text{obs}}$ approaches infinity, the posterior distribution (for global parameters) approximates a normal distribution around the posterior mean. This theorem acts as a bridge between frequentist and Bayesian inference. Hence, in our first simulation study, we use the posterior means of BBVI as point estimates and asses their asymptotics in terms of bias and empirical standard error (EmpSE) over different samples sizes and regression models. The objectives of this study are multifaceted. First, we try to shed light on the asymptotics of the posterior means from BBVI over both small and larger sample sizes. Furthermore, authors have previously illustrated that VI yields consistent parameter estimates, as $n_{\text{obs}}$ approaches infinity, when considering the posterior means as point estimates [@You2014]. Hence, we can rely on the behavior exhibited in large sample sizes as an indication of whether the implementation functions as anticipated. Lastly, we also conducted the simulation study with MCMC to assess the relative performance of BBVI. This might enable us to identify circumstances where the application of BBVI is safe and reliable, especially concerning the sample size.

We will conduct this study for two well-known models: the Bayesian linear regression model ($M_{1}$) and the Bayesian logistic regression model ($M_{2}$) (see @sec-binf). For the data-generating mechanism, we generate pseudo-random data for one covariate from a uniform distribution, parameterized by its minimum and maximum value. Specifically, we use $x \sim \mathcal{U}(-3,3)$. Afterwards, we genetrate the data for the linear predictors by applying the corresponding parametric functions on the random covariate vector. Finally, we obtain samples from the response distribution. For $M_{1}$, this involves sampling noise from a zero-mean normal distribution, parameterized by its scale with $\sigma = 1$, resulting in $\epsilon \sim \mathcal{N}(0,1)$, and adding this noise to the location. On the other hand, for $M_{2}$, we utilize the logit link function to directly sample from a Bernoulli distribution. We employ the following parametric functions for the study:

1. Linear: $f(x) = \beta_{0} + \beta_{1}x$
2. Quadratic $f(x) = \beta_{0} + \beta_{1}x + \beta_{2}x^{2}$

The parameterizations for the corresponding functions are given in Table \ref{tbl-1}.

\begin{table}[htbp]
\centering
\caption{Parameterization of the data-generating functions, for the consistency study.}
    \begin{tabular}{ll}
        \toprule
        & $\betavec$ \\
        \midrule
        Linear & $[1.0, 2.0]^{\mathrm{T}}$ \\
        Quadratic & $[3.0, 0.2, -0.5]^{\mathrm{T}}$. \\
        \bottomrule
    \end{tabular}
\label{tbl-1}
\end{table}

We use the quadratic function for the location of $M_{1}$ and the linear function for the log-odds of $M_{2}$. The choice for the parameterization of the quadratic function was based on assessing plots. Furthermore, the parameterization of the linear function was chosen to achieve a relatively balanced binary response vector for $M_{2}$, where, we obtain with the current parameterization $\frac{1}{n_{\text{obs}}}\sum_{i}^{n_{\text{obs}}}y_{i} \approx 0.58$. To assess the convergence over varying samples sizes $n_{\text{obs}}$, we decide for the following five samples sizes: 50, 100, 500, 1000, 5000. This allows us to study the small sample size behavior as well as large sample asymptotics of the method.

Our Estimands in this study are the fixed known coefficients in Table \ref{tbl-1}, and additionally $\sigma = 1$ for $M_{1}$. The method under investigation is the BBVI algorithm (see Alogrithm \ref{algo}) implemented in `tigerpy`. Furthermore, for evaluating the comparative performance, we employ the No U-turn sampler (NUTS) within MCMC for all model parameters in the software package `liesel` [@Riebl2022]. We use an "arbitrary" initialization of the variational parameters and set $\muvec_{j} = \zerovec$ for all coefficients in the linear predictor and $\muvec_{j} = \ln(10)$ for the scale ($\sigma$). For the precision, we use $\diag(\Lbold_{j}) = 2$ for the scale and $\diag(\Lbold_{j}) = \onevec$ else. Additionally, we use a batch-size of 36, 80% of the data for training, 64 samples from the variational distribution, a max epoch size of 250 and a learning rate of 1e-2. Initial investigations showed that using 250 epochs yielded converging ELBOs even for the smaller sample sizes. Moreover, we also use a new seed for each BBVI run as well as for the data-generating mechanism. For MCMC, we decide to use a burn-in of 1000 samples and then obtain 1000 samples from the Markov chain. 

In this context, our performance measures^[See @Morris2019 [p. 2086, Table 6] for a definition of common performance measures, including those that we used.] include bias and EmpSE. Setting an appropriate number of simulation repetitions $n_{\text{sim}}$ in simulation studies is crucial. We aim for Monte Carlo SEs to be less than 0.1. Initial investigations show that this is broadly the case for $n_{\text{sim}}=100$. Thus we conservatively use $n_{\text{sim}}=200$. The simulation study yielded an outcome without any errors or missing estimators. All results for BBVI are included in this Section, while the results for MCMC are included in [Appendix @sec-mcmc-cons].

Kernel density plots depicting the posterior means for the parameters of $M_{1}$ can be found in @fig-kdeplot-loc and @fig-kdeplot-scale. Notably, for small sample sizes, a considerable variation is evident in our posterior mean estimations. Additionally, the distributions for $\beta_{0}$ and $\beta_{2}$ do not center around their true estimands. Nevertheless, as the sample sizes increase, a convergence to the true estimands becomes apparent. Specifically, after a sample size of 500, our estimators appear to be closely distributed around the true value. In the kernel density estimates of the scale $\sigma$, shown in @fig-kdeplot-scale, we observe a large spread for low sample sizes. Furthermore, the distribution appears to be prominently right skewed for low sample sizes. Again, for a sample size $n_{\text{obs}} \geq 500$ we observe a close alignment around the true estimand.

![Kernel density for the posterior means of the location parameters of $M_{1}$, true parameters given by $\betavec = [3.0, 0.2, -0.5]^{\mathrm{T}}$ (grey dashed lines).](assets/plots/plot8.pdf){width=80% fig-scap="Kernel density for the location parameters of $M_{1}$." #fig-kdeplot-loc}

![Kernel density for the posterior means of the scale parameter of $M_{1}$, true parameter given by $\sigma=1.0$ (grey dashed line).](assets/plots/plot9.pdf){width=80% fig-scap="Kernel density for the scale parameter of $M_{1}$." #fig-kdeplot-scale}

Ideally, we would anticipate observing zero bias across various $n_{\text{obs}}$ sizes, accompanied by a reduction in EmpSE for growing $n_{\text{obs}}$ for our performance measure. Table \ref{tbl-2} depicts the performance measures for the parameters of $M_{1}$. Interestingly, especially for smaller sample sizes, we observe a sizeable bias for $\beta_{0}$ and $\sigma$, but the bias vanishes in size for growing samples sizes. Notably, we still observe a significant bias for $\sigma$ with $n_{\text{obs}}=5000$. Although the bias for $\beta_{2}$ is small in size, the respective 95\% confidence intervals (CI)s do not cover 0 for $n_{\text{obs}}\leq 1000$. For the EmpSE, we observe rather large values for $\beta_{0}$ and $\sigma$. However, all EmpSEs decrease for growing sample sizes.

\begin{table}[htbp]
\centering
\caption{Simulation results for the parameters of $M_{1}$, using BBVI.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} & \multicolumn{5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11} 
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{-0.4514} & \textbf{-0.2191} & \textbf{-0.0428} & -0.0068 & 0.0027 & 0.4303 & 0.2607 & 0.0950 & 0.0604 & 0.0370 \\[-4pt]
         & {\tiny(0.0304)} & {\tiny(0.0184)} & {\tiny(0.0067)} & {\tiny(0.0043)} & {\tiny(0.0026)} & {\tiny(0.0216)} & {\tiny(0.0131)} & {\tiny(0.0048)} & {\tiny(0.0030)} & {\tiny(0.0019)} \\
        $\beta_{1}$ & 0.0118 & 0.0000 & 0.0006 & -0.0009 & -0.0010 & 0.0918 & 0.0586 & 0.0302 & 0.0240 & 0.0183 \\[-4pt]
        & {\tiny(0.0065)} & {\tiny(0.0041)} & {\tiny(0.0021)} & {\tiny(0.0017)} & {\tiny(0.0013)} & {\tiny(0.0046)} & {\tiny(0.0029)} & {\tiny(0.0015)} & {\tiny(0.0012)} & {\tiny(0.0009)}\\
        $\beta_{2}$ & \textbf{0.0907} & \textbf{0.0381} & \textbf{0.0084} & \textbf{0.0029} & -0.0001 & 0.0922 & 0.0571 & 0.0262 & 0.0176 & 0.0107 \\[-4pt]
        & {\tiny(0.0065)} & {\tiny(0.0040)} & {\tiny(0.0019)} & {\tiny(0.0012)} & {\tiny(0.0008)} & {\tiny(0.0046)} & {\tiny(0.0029)} & {\tiny(0.0013)} & {\tiny(0.0009)} & {\tiny(0.0005)} \\
        $\sigma$ & \textbf{0.2207} & \textbf{0.0821} & \textbf{0.0065} & -0.0037 & \textbf{-0.0042} & 0.2939 & 0.1011 & 0.0371 & 0.0299 & 0.0208 \\[-4pt]
        & {\tiny(0.0208)} & {\tiny(0.0072)} & {\tiny(0.0026)} & {\tiny(0.0021)} & {\tiny(0.0015)} & {\tiny(0.0147)} & {\tiny(0.0051)} & {\tiny(0.0019)} & {\tiny(0.0015)} & {\tiny(0.0010)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}% 
}
\label{tbl-2}
\end{table}

For the parameters of $M_{2}$, we also observe a convergence towards the true estimands. When comparing @fig-kdeplot-loc with @fig-kdeplot-logit, the latter exhibits a wider spread. Additionally, for @fig-kdeplot-logit, we observe, that the distributions appear to be slightly right-skewed. Even for large $n_{\text{obs}}$, the empirical distribution is not centered around the true value.

![Kernel density for the posterior means of the logit parameters of $M_{2}$, true parameters given by $\betavec = [1.0, 2.0]^{\mathrm{T}}$ (grey dashed lines).](assets/plots/plot10.pdf){width=80% fig-scap="Kernel density for the logit parameters of $M_{2}$." #fig-kdeplot-logit}

The findings above are validated in Table \ref{tbl-3}. We detect a considerable positive bias in the parameters of $M_{2}$, which gradually decreases as the sample size increases. For both parameters, we observe several instances where 0 is not covered by the respective 95\% CIs, even for larger sample sizes. Research generally showed that logistic regression demonstrates bias in smaller sample sizes when using maximum likelihood estimation [@Nemes2009], which partly helps in explaining the observed results. Nevertheless, even for an observation size of 5000, we observe a small but significant bias. It remains to be explored whether this bias completely diminishes for even larger observation sizes. The EmpSEs once more demonstrate a convergence toward zero. However, the EmpSEs in Table \ref{tbl-1} are larger compared Table \ref{tbl-2}.

\begin{table}[htbp]
\footnotesize
\centering
\caption{Simulation results for the parameters of $M_{2}$, using BBVI.}
\resizebox{\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
         & \multicolumn{5}{c}{Bias} &\multicolumn {5}{c}{EmpSE} \\
        \cmidrule(lr){2-6} 
        \cmidrule(lr){7-11}
        $n_{\text{obs}}$ & 50 & 100 & 500 & 1000 & 5000 & 50 & 100 & 500 & 1000 & 5000 \\
        \midrule
        $\beta_{0}$ & \textbf{0.1774} & 0.0663 & 0.0042 & -0.0231 & \textbf{0.0179} & 0.7487 & 0.5414 & 0.2567 & 0.2042 & 0.1139 \\[-4pt]
        & {\tiny(0.0529)} & {\tiny(0.0383)} & {\tiny(0.0182)} & {\tiny(0.0144)} & {\tiny(0.0081)} & {\tiny(0.0375)} & {\tiny(0.0271)} & {\tiny(0.0129)} & {\tiny(0.0102)} & {\tiny(0.0057)} \\
        $\beta_{1}$ & \textbf{0.3617} & \textbf{0.1845} & 0.0322 & \textbf{0.0388} & \textbf{0.0488} & 0.9235 & 0.6648 & 0.2996 & 0.2129 & 0.1123 \\[-4pt]
        & {\tiny(0.0653)} & {\tiny(0.0470)} & {\tiny(0.0212)} & {\tiny(0.0151)} & {\tiny(0.0079)} & {\tiny(0.0463)} & {\tiny(0.0333)} & {\tiny(0.0150)} & {\tiny(0.0107)} & {\tiny(0.0056)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \footnotesize	
      \item Corresponding Monte Carlo SEs are provided below in parentheses; Bias estimates that do not cover 0 in their 95\% CI are shown in bold; $n_{\text{sim}}=200$.
    \end{tablenotes}
\end{threeparttable}%
}
\label{tbl-3}
\end{table}

Finally, we compare the performance measures obtained from BBVI with those derived from MCMC. For model $M_{1}$	​, we observe that MCMC produces unbiased estimates, even for low sample sizes (refer to Table \ref{tbl-5}). Additionally, all biases are relatively small in magnitude, a contrast to our findings for BBVI. Moreover, the EmpSEs from BBVI are generally larger than those obtained from MCMC. When comparing both performance measures, BBVI seems to yield fairly similar results for $n_{\text{obs}} \geq 500$.

The results for $M_{2}$ using MCMC are depicted in Table \ref{tbl-6}. We find that MCMC also obtains sizeable and significant biases for low to medium samples sizes. However, the biases vanishes for $n_{\text{obs}} \geq 1000$. Comparing the EmpSEs we find that MCMC obtains larger EmpSEs for low samples sizes. In general, it seems that MCMC exhibits faster and more consistent convergence compared to BBVI.

In this study, we demonstrate that while the posterior means of BBVI exhibit substantial inaccuracies in small sample sizes, they boradly converge towards the fixed true parameters. However, even with larger sample sizes, we sometimes observe a persistent significant bias, albeit diminished in size. Therefore, it would be interesting to further investigate the behaviour for larger sample sizes to observe if the posterior means from BBVI fully converge towards the fixed estimands. Moreover, we find that with a sample size of 500, BBVI produces by and large comparable results to MCMC. Interestingly, in logistic regression with a sample size of 50, BBVI demonstrates a narrower spread of posterior mean estimates compared to MCMC. Finally, there is no indication of an implementation error causing systematic errors and, consequently, sizeable biased estimates over increasing sample sizes. 

## Posterior distribution study

In the forthcoming study, our goal is to contrast the posterior distributions obtained by fitting a smooth curve using Bayesian P-splines via both BBVI and MCMC techniques. We initiate the study by visually assessing the fits and comparing the posterior samples derived from BBVI and MCMC. While this initial evaluation provides us with a first impression, our intention is to rigorously quantify the disparities between the two methods. For this, we measure the distance between the posterior samples from BBVI and MCMC for each parameter block using the Wassterstein distance [@Kantorovich1960]. Are the posterior distributions closely comparable, or does BBVI yield notably different outcomes compared to MCMC?

The data-generating mechanism is given by 

$$
\begin{split}
    y_{i} | x_{i} &\sim \mathcal{N}(f(x_{i}), 1.5^{2}) \\
    f(x_{i}) &= 3.0 + 1.75 \sin(1.5x_{i}) \\
    x_{i} &\sim \mathcal{U}(-10,10), \ i=1, \dots, 1000,
\end{split}
$$

which we denote in the following as data generating process (DGP). For the observation sample size we use $n_{\text{obs}}=1000$. Furthermore, we slightly heightened the frequency and amplitude of the $\sin$ function to augment the complexity of estimating the function. The GDP is depicted in @fig-sim2-dgp.

The objective of this study is to visually and quantitatively compare the posterior distributions generated by MCMC and BBVI methods. To conduct the visual assessment, we employed four independent MCMC chains and conducted four separate BBVI runs to estimate the smooth function for one dataset. Following the optimization of the variational distribution, we can sample from it, enabling a comparative analysis of the posterior distributions via kernel density plots. After this initial investigation, we quantify the closeness of the distributions by generating $n_{\text{sim}}=100$ datasets for which we estimate the smooth function with BBVI and MCMC. For the quantification of the distance between the posterior distributions we utilize the Wasserstein distance [@Kantorovich1960] with the squared euclidean distance ($W_{2}$). This allows us to empirically quantify the closeness of our variational approximation. The metric, for the squared euclidean distance, is defined as

\begin{align*}
    W_{2}(P, Q) &= \underset{\pi}{\inf} \left( \frac{1}{n} \sum_{i=1}^{n} || X_{i} - Y_{\pi(i)} ||^{2} \right)^{\frac{1}{2}}.
\end{align*}

$P$ and $Q$ are two empirical distributions, in our case the posterior distributions from MCMC and BBVI. Furthermore, the infimum is considered over all permutations $\pi$ of the $n$ samples. To efficiently solve this problem in cubic time $\left( \mathcal{O}(n^{3}) \right)$, we need to employ linear programming techniques and reframe it as an optimal transport problem. For the estimation of the Wasserstein distance, we utilize the POT package [@pot]. To maintain a manageable level of complexity, we chose to compare 1000 samples from both BBVI and MCMC.

We once again utilize `tigerpy` for BBVI and `liesel` [@Riebl2022] for MCMC estimation, respectively. For both models, we specify a linear predictor for the location with a fixed intercept $\beta_{0}$ and a smooth effect through Bayesian P-splines, parameterized by $\gammavec$ and $\tau^{2}$. We opt for 20 knots, a degree of 3 and a random walk order of 2. In BBVI, we again do not use a model specific initialization and set $\muvec_{j}=\zerovec$, $\diag(\Lbold_{j}) = 2$ for the scale parameter ($\sigma$) and $\diag(\Lbold_{j}) = \onevec$ otherwise. This is done to fairly compare BBVI with MCMC in `liesel`, where we also do not provide a model specific initialization. Furthermore, we fix the epoch size to 500, the learning rate to 1e-2, the batch-size to 256, set the training share to 0.8 and use 64 samples to evaluate the Monte Carlo integral. For the MCMC algorithm, we use a Gibbs sampler kernel for the inverse smoothing parameter $\tau^{2}$ and a NUTS kernel for all other parameter blocks. Finally, we use 1000 samples for the burn-in and then obtain 1000 samples from the Markov chain.

The fits resulting from one run of BBVI and one chain from MCMC, utilizing the posterior means, are presented in @fig-sim2-post-means. Both methods yield surprisingly similar fits, successfully capturing the smooth function. Despite BBVI approximating the posterior distribution with a simpler distribution, it is interesting to observe its ability to infer the posterior means effectively. Overall, BBVI generally presents a slightly inferior fit. Specifically, at the right boundary, BBVI produces a less accurate fit compared to MCMC. 

::: {layout="[[50,-5,50]]"}

![The DGP, mean visualized by the solid line and 95% confidence band by the dashed line.](assets/plots/plot11.pdf){fig-scap="Plot of the DGP." #fig-sim2-dgp}

![The DGP and the estimated smooth functions of BBVI and MCMC using the posterior mean.](assets/plots/plot12.pdf){fig-scap="Plot of the estimated smooth functions." #fig-sim2-post-means}

:::

Kernel density plots representing the posterior samples of the model parameters of the four MCMC chains and four BBVI runs are available from @fig-sim2-fixed to @fig-sim2-scale. Once more, we notice similar posterior distributions for both inference methods. However, the resulting posterior distributions from BBVI for different runs appear to exhibit a wider degree of variability. Noticeable differences emerge for smooth coefficients on the left boundary (see @fig-sim2-smooth, upper left). MCMC appears to estimate the posterior with considerably higher uncertainty compared to BBVI for these coefficients. It is important to note that with BBVI, there is a possibility of getting trapped in a bad local optimum leading to a considerable difference between the posterior distributions of the two methods. Nevertheless, for the four runs that we estimated this was not the case. 

![Kernel density for the posterior samples of the fixed intercept $\beta_{0}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot13.pdf){width=60% fig-scap="Kernel density for the posterior samples of the fixed intercept." #fig-sim2-fixed}

![Kernel density for the posterior samples of selected internal spline coefficients $\tilde{\gammavec}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot14.pdf){width=60% fig-scap="Kernel density for the posterior samples of selected spline coefficients." #fig-sim2-smooth}

We observe a slight variation between BBVI and MCMC concerning the inverse smoothing parameter $\tau^{2}$. This discrepancy may be attributed to our use of a Gibbs sampler with an inverse gamma distribution for the full conditional of $\tau^{2}$ in MCMC, while we only use a log-normal distribution as a factor for $\tau^{2}$ in the variational distribution. Notably, the inverse gamma distribution demonstrates a heavier right tail compared to the log-normal distribution, as illustrated in @fig-sim2-tau2. Although the posterior distributions of the scale parameter seem relatively similar in both methods, @fig-sim2-scale highlights a significantly broader range of variability in the posterior distributions for BBVI.

![Kernel density for the posterior samples of the inverse smoothing parameter $\tau^{2}$, using 4 randomly selected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot15.pdf){width=60% fig-scap="Kernel density for the posterior samples of the inverse smoothing parameter." #fig-sim2-tau2}

![Kernel density for the posterior samples of the scale $\sigma$, using 4 randomly slected runs from BBVI (red) and 4 chains from MCMC (blue).](assets/plots/plot16.pdf){width=60% fig-scap="Kernel density for the posterior samples of the scale parameter." #fig-sim2-scale}

Table \ref{tbl-4} depicts the statistics of the Wasserstein distance between the posterior samples from MCMC and BBVI for $n_{\text{sim}}=100$. The posterior distributions of the intercept $\beta_{0}$ and the scale $\sigma$ are notably similar in both methods. However, distinctions arise for the smooth coefficients $\gammavec$ and the inverse smoothing parameter $\tau^{2}$. For both coeffecients the $W_{2}$ metrics are on average $> 1$ and right skewed. The worst variational approximation seems to be the distribution for $\tau^{2}$ where we even observe values $> 4$. These findings can be also visually assesed in the box-plots of @fig-sim2-wd.

\begin{table}[htbp]
\centering
\caption{Simluation results of the Wasserstein distance ($W_{2}$).}
\begin{threeparttable}
    \begin{tabular}{lllll}
        \toprule
         & \multicolumn{4}{c}{Parameters} \\
        \cmidrule(lr){2-5} 
         & $\beta_{0}$ & $\gammavec$ & $\sigma$ & $\tau^{2}$ \\
        \midrule
        Mean & 0.0270 & 1.6377 & 0.0260 & 1.9650 \\[-4pt]
        & {\tiny(0.0035)} & {\tiny(0.0484)} & {\tiny(0.0031)} & {\tiny(0.1179)} \\
        Median & 0.0180 & 1.5148 & 0.0161 & 1.6520 \\[-4pt]
        & {\tiny(0.0044)} & {\tiny(0.0606)} & {\tiny(0.0039)} & {\tiny(0.1478)} \\
        $Q_{0.25}$ & 0.0111 & 1.3805 & 0.0098 & 1.1773 \\[-4pt]
        & {\tiny(0.0048)} & {\tiny(0.0659)} & {\tiny(0.0043)} & {\tiny(0.1607)} \\
        $Q_{0.75}$ & 0.0311 & 1.6095 & 0.0269 & 2.3657 \\[-4pt]
        & {\tiny(0.0048)} & {\tiny(0.0659)} & {\tiny(0.0043)} & {\tiny(0.1607)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize	
        \item Corresponding Monte Carlo SEs are provided below in parentheses; $n_{\text{sim}}=100$.
    \end{tablenotes}
\end{threeparttable}
\label{tbl-4}
\end{table}

![Box plots displaying the Wasserstein distance for the different model parameters.](assets/plots/plot17.pdf){width=80% fig-scap="Box plots of the Wasserstein distance." #fig-sim2-wd}

In this study, our findings showcased the capability of BBVI to generate rather similar posterior distributions for the examined model parameters. Nevertheless, due to the absence of comparative studies, interpreting these numbers remains difficult. Thus, our approach to empirically quantify the proximity between our variational distribution and the posterior from MCMC could serve as a procedure for future research endeavors focused on evaluating the effectiveness of variational approximations tailored to specific models. It is advisable for subsequent studies to aim for comparative analyses, comparing variational approximations against asymptotically exact methods like MCMC. This could complement theoretical findings by providing empirical insights into the accuracy of different variational approximations.

## Runtime study 

We'll conclude by comparing the runtime of our BBVI implementation with that of MCMC in `liesel`. BBVI is typically advocated due to the limited scalability of MCMC for complex models with large datasets. Thus, when opting for a less accurate method compared to MCMC, which guarantees exact results over extensive sampling, there should be a noticeable advantage in terms of scalability and thus runtime. We will compare the runtime for BBVI and MCMC using a common dataset for semiparameteric distributional regression. 

The dataset we are utilizing is the Dutch boys dataset (dbbmi), which contains age and body mass index (bmi) data of Dutch boys aged between 0 and 21 years [@Fredriks2000; @Rigby2005]. Exploring the relationship between age (independent variable) and bmi (dependent variable) is here of particular interest. The data set contains $n_{\text{obs}}=7294$ observations for the two variables. Notably, this dataset portrays a highly non-linear relationship between age and BMI (see @fig-sim3-fit). There appears to be an increasing spread in BMI with age, suggesting the presence of heteroskedasticity. Hence, the use of a location-scale regression model for this data set seems reasonable, incorporating both a smooth effect on the location and on the scale. Furthermore, we decide to use a normal distribution as the response distribution. The linear predictors are specified as 

$$
\eta_{i,l} = \beta_{0} + f(\text{age}_{i}), \ l = 1,2.
$$

Our estimands are the runtimes for each method, respectively. Runtime includes the model construction time as well as computation time for the inference algorithm. The methods under consideration are, as always, BBVI (`tigerpy`) and MCMC (`liesel`). In this study, we also made shure, through initial studies, that both algorithms yielded converging posterior results with their choosen specification. For BBVI, we use a MAP optimization as pre-training, finalized by a Laplace approximation (learning rate of 0.01 and a batch size of 64), which serves as the starting point for the ELBO optimization. Afterwards, we optimize the ELBO with a learning rate of 0.01, a batch size of 256, a training share of 0.8, and use 64 samples from the variational distribution over 250 epochs. In MCMC, we use Gibbs Kernels, with inverse gamma distributions, for the inverse smoothing parameters and NUTS Kernels for other parameters. The length of the burn-in was set to 1000, and we sample subsequently for 1000 iterations. The performance measure constitutes the mean runtime over $n_{\text{sim}}=10$ iterations.

@fig-sim3-fit displays the resulting fits of the smooth function from one run of MCMC and BBVI using the posterior means. We observe similar fits with deviations on the right boundary where the data is sparse. Besides, MCMC obtains a slightly smoother curve for the 95\% confidence band compared to BBVI, suggesting a smaller inverse smoothing parameter for the scale.

![Fit of the smooth curves for the location (solid) and 95\% confidence bands (dashed) using the posterior means from MCMC (red) and BBVI (green).](assets/plots/plot18.pdf){width=60% fig-scap="Fit of the smooth curves in the Dutch boys dataset." #fig-sim3-fit}

The runtime results are presented in Table \ref{tbl-5}. It is evident that `tigerpy` reduces the runtime by more than a factor of 10. This highlights the potential of BBVI to substantially improve runtimes, even for simpler models with medium-sized datasets. 

\begin{table}[htbp]
\centering
\caption{Runtime comparison.}
\begin{threeparttable}
    \begin{tabular}{lrr}
        \toprule
         & \multicolumn{2}{c}{Packages} \\
        \cmidrule(lr){2-3} 
         & tigerpy & liesel \\
        \midrule
        Mean & 0.4903 & 7.0682 \\[-4pt]
        & {\tiny(0.0218)} & {\tiny(0.1781)} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \footnotesize	
        \item Runtime is displayed in min.; Time needed for model construction is included; Corresponding Monte Carlo SEs are provided below in parentheses; $n_{\text{sim}}=10$.
    \end{tablenotes}
\end{threeparttable}
\label{tbl-5}
\end{table}