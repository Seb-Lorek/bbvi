% --- Coverpage ---%
\thispagestyle{empty}
\topskip0pt

% Vertical centering (start)
\vspace*{\fill}

% University logo
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{img/logo_uni_goe.pdf}
\end{figure}

\bigskip

\begin{center}

% Title
\rule{\linewidth}{0.5mm}

{\Large \textbf{A Stochastic Variational Inference Approach for Semiparametric
                Distributional Regression}}

\rule{\linewidth}{0.5mm} \\

\end{center}

\bigskip

% Author
\begin{center}

by \\

\bigskip

Lorek, Sebastian \\
Matriculation number: 20311558

\end{center}

\bigskip

% Degree
\begin{center}

A Master's Thesis \\
Submitted to the Chair of Statistics \\
Faculty of Business and Economics, University of GÃ¶ttingen \\
In Fulfillment of the Requirements \\
For a Master's Degree in Applied Statistics\\
First Supervisor: Prof. Dr. Thomas Kneib \\
Second Supervisor: Gianmarco Callegher \\
December, 2023 \\

\end{center}


% Vertical centering (end)
\vspace*{\fill}

% For TOC etc. already
\clearpage
\pagenumbering{Roman}

% --- Abstract --- %

\section*{Abstract}

The thesis explores variational inference as a viable alternative tool for 
tackling Bayesian inference problems, allowing for the approximation of 
posterior distributions through optimization. Variational inference is applied 
to Bayesian (semiparameteric) 
disitrubtional regression models and several simulation studies traget 
different statistical aspects of the implemented inference algorithm. The "black-box" 
variational inference algorithm has been implemented in \verb|python| in 
combination with a model building library that uses the idea of probabilistic 
graphical models to construct Bayesian semiparametric distributional regression 
models and retrieve node information during the process of inference. A simulation 
study shows that the implemented variational inference algorithm results in 
consistent parameter estimates when considering the posterior mean. 
Moreover analyzes the thesis in a simulation study the posterior distributions 
of Markov chain Monte Carlo and "black-box" variational inference and shows 
that variational inference obtains rather similar posterior distributions. 
Finally variational inference indicates, in a further study, the ability to 
approximate the posterior well even after a low number of iterations. 
This is in contrast to Markov Chain Monte Carlo methods which usually require a 
large number of iterations to obtain a good approximation of the posterior.

\clearpage

% --- TOC --- %
\tableofcontents
\clearpage

% --- LOF --- %
\listoffigures

% --- LOA --- %
\listofalgorithms 

% --- LOT --- %
\listoftables

% --- List of abbreviations --- %
\section*{List of Abbreviations}

\begin{tabular}{@{} l @{\hskip 1in} l}
  VI & Variational inference \\
  SVI & Stochastic variational inference \\
  MCMC & Markov chain Monte Carlo \\
  KL & Kullback-Leibler \\
  GAMLSS & Generalized additive models for location, scale and shape \\
  CAVI & Coordinate ascent variational inference \\
  SGD & Stochastic gradient descent \\
  BBVI & "Black-box" variational inference \\
  API & Application programming interface \\
  JIT & Just-in-time compilation \\
  CPU & Central processing unit \\
  GPU & Graphical processing unit \\
  TPU & Tensor processing unit \\
  EmpSE & Emprical standard error \\
  SE & Standard error \\
\end{tabular}
