% --- Coverpage ---%
\thispagestyle{empty}
\topskip0pt

% Vertical centering (start)
\vspace*{\fill}

% University logo
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{img/logo_uni_goe.pdf}
\end{figure}

\bigskip

\begin{center}

% Title
\rule{\linewidth}{0.5mm}

{\Large \textbf{A Stochastic Variational Inference Approach for Semiparametric
                Distributional Regression}}

\rule{\linewidth}{0.5mm} \\

\end{center}

\bigskip

% Author
\begin{center}

by \\

\bigskip

Lorek, Sebastian \\
Matriculation number: 20311558

\end{center}

\bigskip

% Degree
\begin{center}

A Master's Thesis \\
Submitted to the Chair of Statistics \\
Faculty of Business and Economics, University of GÃ¶ttingen \\
In Fulfillment of the Requirements \\
For a Master's Degree in Applied Statistics\\
First Supervisor: Prof. Dr. Thomas Kneib \\
Second Supervisor: Gianmarco Callegher \\
December, 2023 \\

\end{center}


% Vertical centering (end)
\vspace*{\fill}

% For TOC etc. already
\clearpage
\pagenumbering{Roman}

% --- Abstract --- %

\section*{Abstract}

This thesis explores variational inference as a viable alternative tool for 
tackling Bayesian inference problems, 
allowing the approximation of posterior distributions by optimization. 
We study "black box" variational inference, in Bayesian semiparametric distributional regression models,
a method enabling inference in numerous probabilistic models without requiring model-specific derivations.
The "black box" variational inference algorithm has been implemented in Python 
within a software package consisting of a model building and inference library, 
which utilize the framework of probabilistic graphical models.
A simulation study shows that the implemented variational inference algorithm yields generally 
consistent estimators when considering the posterior means of the model parameters for common regression models. 
Furthermore, the thesis conducts a simulation study to compare the posterior distributions 
of Markov chain Monte Carlo with "black box" variational inference. It demonstrates that 
the choosen mean-field variational family over parameter blocks obtains similar posterior distributions to those in MCMC.
The implemented variational inference algorithm also shows the ability to solve inference in a
Bayesian semiparametric distributional regression model faster than an implementation of Markov chain Monte Carlo. 

\clearpage

% --- TOC --- %
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage

% --- LOF --- %
\listoffigures

% --- LOA --- %
\listofalgorithms 

% --- LOT --- %
\listoftables

% --- List of abbreviations --- %
\section*{List of Abbreviations}

\begin{tabular}{@{} l @{\hskip 1in} l}
  VI & Variational inference \\
  MCMC & Markov chain Monte Carlo \\
  KL & Kullback-Leibler \\
  BBVI & "Black box" variational inference \\
  GAMLSS & Generalized additive models for location, scale and shape \\
  DAG & Directed acyclic graph \\ 
  SVI & Stochastic variational inference \\
  CAVI & Coordinate ascent variational inference \\
  SGD & Stochastic gradient descent \\
  MAP & Maximum a posteriori \\
  API & Application programming interface \\
  JIT & Just-in-time compilation \\
  CPU & Central processing unit \\
  GPU & Graphical processing unit \\
  TPU & Tensor processing unit \\
  RS & Rigby and Stasinopoulos \\
  CG & Cole and Green \\
  EmpSE & Empirical standard error \\
  SE & Standard error \\
  DGP & Data generating process \\
  NUTS & No U-Turn sampler \\
  VCM & Varying coefficient models \\
\end{tabular}
