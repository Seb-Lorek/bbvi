% --- Coverpage ---%
\thispagestyle{empty}
\topskip0pt

% Vertical centering (start)
\vspace*{\fill}

% University logo
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{img/logo_uni_goe.pdf}
\end{figure}

\bigskip

\begin{center}

% Title
\rule{\linewidth}{0.5mm}

{\Large \textbf{A Stochastic Variational Inference Approach for Semiparametric
                Distributional Regression}}

\rule{\linewidth}{0.5mm} \\

\end{center}

\bigskip

% Author
\begin{center}

by \\

\bigskip

Lorek, Sebastian \\
Matriculation number: 20311558

\end{center}

\bigskip

% Degree
\begin{center}

A Master's Thesis \\
Submitted to the Chair of Statistics \\
Faculty of Business and Economics, University of GÃ¶ttingen \\
In Fulfillment of the Requirements \\
For a Master's Degree in Applied Statistics\\
First Supervisor: Prof. Dr. Thomas Kneib \\
Second Supervisor: Gianmarco Callegher \\
December, 2023 \\

\end{center}


% Vertical centering (end)
\vspace*{\fill}

% For TOC etc. already
\clearpage
\pagenumbering{Roman}

% --- Abstract --- %

\section*{Abstract}

This thesis explores variational inference as a viable alternative tool for 
tackling Bayesian inference problems, 
allowing for the approximation of posterior distributions through optimization. 
We study "black box" variational inference, in Bayesian semiparametric distributional regression models,
a method enabling inference in numerous probabilistic models without requiring model-specific derivations.
The "black box" variational inference algorithm has been implemented in Python 
within a software package consisting of a model building and inference library, 
which leverage the framework of probabilistic graphical models.
A simulation study shows that the implemented variational inference algorithm results in 
consistent estimates when considering the posterior means of the model parameters. 
Furthermore, the thesis conducts a simulation study to compare the posterior distributions 
of Markov chain Monte Carlo and "black box" variational inference. It demonstrates that 
the mean-field variational family over parameter blocks performs more effectively for certain blocks compared to others.
Finally variational inference indicates, the ability to 
approximate the posterior well even after a low number of optimization iterations. 
This is in contrast to Markov Chain Monte Carlo methods, which usually require a 
large number of sampling iterations to obtain a good approximation of the posterior.

\clearpage

% --- TOC --- %
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage

% --- LOF --- %
\listoffigures

% --- LOA --- %
\listofalgorithms 

% --- LOT --- %
\listoftables

% --- List of abbreviations --- %
\section*{List of Abbreviations}

\begin{tabular}{@{} l @{\hskip 1in} l}
  VI & Variational inference \\
  MCMC & Markov chain Monte Carlo \\
  KL & Kullback-Leibler \\
  BBVI & "Black box" variational inference \\
  GAMLSS & Generalized additive models for location, scale and shape \\
  DAG & Directed acyclic graph \\ 
  SVI & Stochastic variational inference \\
  CAVI & Coordinate ascent variational inference \\
  SGD & Stochastic gradient descent \\
  API & Application programming interface \\
  JIT & Just-in-time compilation \\
  CPU & Central processing unit \\
  GPU & Graphical processing unit \\
  TPU & Tensor processing unit \\
  RS & Rigby and Stasinopoulos \\
  CG & Cole and Green \\
  EmpSE & Empirical standard error \\
  SE & Standard error \\
  DGP & Data generating process \\
  IWLS & Iterated weighted least squares \\
  NUTS & No U-Turn sampler \\
\end{tabular}
